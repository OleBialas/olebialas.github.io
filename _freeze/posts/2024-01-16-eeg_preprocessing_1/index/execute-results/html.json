{
  "hash": "2f96824622bb1015297c1f0047c3771c",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"EEG preprocessing I: detrending, denoising and referencing\"\ndescription: \"Preprocessing is an important and controversial topic in EEG research. Here, I discuss it's necessity and present a minimal preprocessing pipeline that deals with the most common sources of noise while avoiding to distort the data. I demonstrate each step using publicly available data.\"\ndate: \"2024-01-16\"\nimage: featured.png\nimage-alt: |\n  `matplotlib` plot of an event-related potential\naliases:\n  - /post/eeg-preprocessing-1/index.html\n  - /post/eeg-preprocessing-1.html\nengine: Jupyter\nformat:\n  html: default\n  ipynb: default\nfreeze: auto\n---\n\n**E**lectro**e**ncephalo**g**raphy (EEG) measures brain activity via electrodes on the scalp.\nUnfortunately, those electrodes also picks up other things like **muscle activity** and electromagnetic **interference** that are **orders of magnitude** larger than neural responses. There is a vast literature and **no consensus** on how to deal with this problem.\n\n# To preprocess or not to preprocess?\nA recent paper which got some attention argued that sophisticated preprocessing pipelines are **altering data for the worse** and that *\"EEG is better left alone\"* [^1].\nWhile I generally agree that preprocessing should be kept **as minimal as possible**, I think there are some issues with this generalized statement. \n\nThe paper considers a preprocessing method to be effective if it increases the number of **channels significantly differing** between conditions. However, without a known **ground truth**, we can't distinguish true effects from spurious findings that may result from **distorting** the data. Thus, increased significance does not necessarily prove a method's superiority!\n\nAlso, the paper does not consider data of **varying quality**.\nEven if certain methods won't improve recordings of high quality, they may still be beneficial if the data is more noisy, for example if it was recorded in a hospital **without sufficient electric shielding**.\n\nFor these reasons, I decided to split this guide into two parts. Part I describes a **minimal set** of preprocessing steps that are necessary for most EEG analyses. The procedures I suggest are **robust** to noise and minimize the **risk of distorting** the data.\nPart II will introduce  additional procedures that may help with more **noisy data**.\n\n# Prerequisites\nTo follow along with the examles, you'll have to install several packages. I recommend creating a new environment and using pip to install:\n\n```\npip install mne meegkit pyprep\n```\n\nWe'll use **sample data** provided by MNE-Python which will be downloaded automatically when you call the `data_path` function for the first time.\nThen, we load the raw data, find **events** in the data (we'll need those later) and remove everything but the EEG channels [^2]. Finally, we **downsample** the data so processing will be quicker.\n\n::: {#6b628f92 .cell execution_count=1}\n``` {.python .cell-code}\nfrom mne.io import read_raw_fif\nfrom mne.datasets.sample import data_path\nfrom mne import find_events\n\nraw = read_raw_fif(data_path() / \"MEG/sample/sample_audvis_raw.fif\")\nevents = find_events(raw)\nraw.pick(picks=\"eeg\")\nraw, events = raw.resample(150, events=events)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nOpening raw data file /home/olebi/mne_data/MNE-sample-data/MEG/sample/sample_audvis_raw.fif...\n    Read a total of 3 projection items:\n        PCA-v1 (1 x 102)  idle\n        PCA-v2 (1 x 102)  idle\n        PCA-v3 (1 x 102)  idle\n    Range : 25800 ... 192599 =     42.956 ...   320.670 secs\nReady.\nFinding events on: STI 014\n320 events found on stim channel STI 014\nEvent IDs: [ 1  2  3  4  5 32]\n```\n:::\n:::\n\n\n# Channel drifts and offsets\nChannels differ in their **conductivity** with the scalp and this conductivity may also change across time, for example if the subject is sweating.\nThis results in different and **drifting baselines** that can overshadow neural activity.\nLet's look at two exemplar channels:\n\n::: {#3c470511 .cell execution_count=2}\n``` {.python .cell-code}\nfrom matplotlib import pyplot as plt\n\nplt.plot(raw.times, raw.get_data()[[3, 50], :].T * 1e6, linewidth=0.4)\nplt.xlabel(\"Time [s]\")\nplt.ylabel(\"Voltage [muV]\");\n```\n\n::: {.cell-output .cell-output-display}\n![Two EEG channels before detrending. Note how the time series are similar but differ in their offset.](index_files/figure-html/cell-3-output-1.png){width=604 height=429}\n:::\n:::\n\n\nA common way to deal with fluctuating baselines is to apply a **high pass filter**, suppressing all fluctuations at frequencies below some threshold (often 0.5 or 1 Hz).\nWhile this effectively removes channel offsets and drifts, it may also **smear** the signal or even introduce **spurious** features [^3].\n\nAn alternative is to detrend the data by fitting a polynomial and subtracting the fit.\nHere, I use an algorithm for **robust detrending** that ignores outliers [^4].\nI apply detrending twice - first using a line and then a **higher-order** polynomial to remove faster baseline fluctuations.\n\n::: {#36111440 .cell execution_count=3}\n``` {.python .cell-code}\nfrom meegkit.detrend import detrend\n\nX = raw.get_data().T  # transpose so the data is organized time-by-channels\nX, _, _ = detrend(X, order=1)\nX, _, _ = detrend(X, order=6)\nraw._data = X.T  # overwrite raw data\nplt.plot(raw.times, X[:, [3, 50]], linewidth=0.4)\nplt.xlabel(\"Time [s]\")\nplt.ylabel(\"Voltage [muV]\");\n```\n\n::: {.cell-output .cell-output-display}\n![The same two EEG channels after detrending. Note how removing the offset makes comparing the signals much easier.](index_files/figure-html/cell-4-output-1.png){width=625 height=429}\n:::\n:::\n\n\n# Removing power line noise\nAnother ubiquitous problem is the presence of power line noise. This takes the form of an oscillation at the frequency of the **alternating current** signal which is **60 Hz** in the US and **50 Hz** in the rest of the world.\nTo see this, we can plot the power spectral density (PSD), which shows the signals **power content** across frequencies.\n\n::: {#e03a3201 .cell execution_count=4}\n``` {.python .cell-code}\nfig, ax = plt.subplots()\npsd = raw.compute_psd()\npsd.plot(axes=ax, average=True);\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nEffective window size : 13.653 (s)\nPlotting power spectral density (dB=True).\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![Power spectral density (PSD) of the EEG recording before filtering. Note the peak at 60 Hz marked by the dashed line.](index_files/figure-html/cell-5-output-2.png){width=600 height=449}\n:::\n:::\n\n\nEven though this recording is very clean, you can make out a little spike in the PSD at 60 Hz.\nThe most common solution to power line noise is to apply a **low pass or notch filter** that removes all activity within the contaminated frequency band.\nHowever, to sharply separate noise and signal frequencies, the filter must have a steep transfer function which results in a long impulse response that introduces **ringing artifacts** into the signal.\n\nAlternatively, one may use a **spatial filter** but this only works if noise and neural signal are **linearly separable**. Here, I use an algorithm that **combines** the advantages of both approaches to remove power line noise while minimizing distortions and loss of data [^5].\n\n::: {#a5dae1d2 .cell execution_count=5}\n``` {.python .cell-code}\nfrom meegkit.dss import dss_line\nX, noise = dss_line(X, fline=60, sfreq=raw.info['sfreq'], nremove=3)\nraw._data = X.T\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPower of components removed by DSS: 0.00\n```\n:::\n:::\n\n\nTo verify that the algorithm worked as expected we visualize the power content of the removed noise. The PSD should be **mostly flat**, except for a spike at the power line frequency.\n\n::: {#8a7c07f9 .cell execution_count=6}\n``` {.python .cell-code}\nfrom mne.io import RawArray\n\nnoise = RawArray(noise.T, raw.info)\npsd_noise = noise.compute_psd()\nfig, ax = plt.subplots()\npsd_noise.plot(axes=ax, average=True);\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCreating RawArray with float64 data, n_channels=60, n_times=41657\n    Range : 0 ... 41656 =      0.000 ...   277.707 secs\nReady.\nEffective window size : 13.653 (s)\nPlotting power spectral density (dB=True).\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![Power spectral density (PSD) of the noise separated by the spatial filter. Note the peak at 60 Hz is more prominent compared to the PSD of the full signal.](index_files/figure-html/cell-7-output-2.png){width=600 height=449}\n:::\n:::\n\n\n# Re-referencing to a robust average\nVoltage is the difference in electric potential between two points. \nThus, the voltage measured at each EEG channel is relative to some **common reference** point.\nBecause manufacturers use different recording references it is usually a good idea to re-reference the signals.\n\nThis is done by simply **subtracting** a channel or combination of channels. While this changes the absolute magnitudes, it does not alter the **relations** between channels. Imagine the peaks and valleys in voltage as a landscape and re-referencing as **changing your standpoint**. Depending on where you stand, any **single point** may be above or below you, but the landscape stays the same!\n\nA common reference choice is to use the **average** of all channels.\nHowever, single \"bad\" channels, containing large artifacts may skew the average and **leak** those artifacts into all other channels.\nTo prevent this from happening, we can **identify and interpolate** those bad channels before computing the average.\n\nI use the random sample consensus (RANSAC) method [^6] to identify bad channels. RANSAC **predicts EEG** channels from their neighbors and marks them as bad if their correlation with that prediction fails to meet some **threshold**.\n\n::: {#76093b15 .cell execution_count=7}\n``` {.python .cell-code}\nfrom pyprep.ransac import find_bad_by_ransac\nimport numpy as np\n\nbads, _ = find_bad_by_ransac(\n     data = raw.get_data(),\n     sample_rate = raw.info['sfreq'],\n     complete_chn_labs = np.asarray(raw.info['ch_names']),\n     chn_pos = np.stack([ch['loc'][0:3] for ch in raw.info['chs']]),\n     exclude = [],\n     corr_thresh = 0.9\n     )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nExecuting RANSAC\nThis may take a while, so be patient...\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<script type=\"application/vnd.jupyter.widget-view+json\">\n{\"model_id\":\"f946be2a914a45bd980337c1a921ac21\",\"version_major\":2,\"version_minor\":0,\"quarto_mimetype\":\"application/vnd.jupyter.widget-view+json\"}\n</script>\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\nRANSAC done!\n```\n:::\n:::\n\n\nBecause we don't actually want to remove anything yet (data rejection will be addressed in the next post), we **copy the data** before interpolating the bad channels.\nThen, we compute the average reference on this cleaned copy and apply it to the original data as a **projection**.\n\n::: {#63f6ba82 .cell execution_count=8}\n``` {.python .cell-code}\nraw_clean = raw.copy()\nraw_clean.info['bads'] = bads\nraw_clean.interpolate_bads()\nraw_clean.set_eeg_reference('average', projection=True)  #compute the reference\nraw.add_proj(raw_clean.info['projs'][0])\ndel raw_clean  # delete the copy\nraw.apply_proj()  # apply the reference\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSetting channel interpolation method to {'eeg': 'spline'}.\nInterpolating bad channels.\n    Automatic origin fit: head of radius 91.2 mm\nComputing interpolation matrix from 51 sensor positions\nInterpolating 9 sensors\nEEG channel type selected for re-referencing\nAdding average EEG reference projection.\n1 projection items deactivated\nAverage reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n1 projection items deactivated\nCreated an SSP operator (subspace dimension = 1)\n1 projection items activated\nSSP projectors applied...\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=8}\n```{=html}\n<script type=\"text/javascript\">\n    // must be `var` (not `const`) because this can get embedded multiple times on a page\nvar toggleVisibility = (className) => {\n\n    const elements = document.querySelectorAll(`.${className}`);\n\n    elements.forEach(element => {\n        if (element.classList.contains(\"mne-repr-section-header\")) {\n            return  // Don't collapse the section header row\n        }\n        element.classList.toggle(\"mne-repr-collapsed\");\n    });\n\n    // trigger caret to rotate\n    var sel = `.mne-repr-section-header.${className} > th.mne-repr-section-toggle > button`;\n    const button = document.querySelector(sel);\n    button.classList.toggle(\"collapsed\");\n\n    // adjust tooltip\n    sel = `tr.mne-repr-section-header.${className}`;\n    const secHeadRow = document.querySelector(sel);\n    secHeadRow.classList.toggle(\"collapsed\");\n    secHeadRow.title = secHeadRow.title === \"Hide section\" ? \"Show section\" : \"Hide section\";\n}\n</script>\n\n<style type=\"text/css\">\n    /*\nStyles in this section apply both to the sphinx-built website docs and to notebooks\nrendered in an IDE or in Jupyter. In our web docs, styles here are complemented by\ndoc/_static/styles.css and other CSS files (e.g. from the sphinx theme, sphinx-gallery,\nor bootstrap). In IDEs/Jupyter, those style files are unavailable, so only the rules in\nthis file apply (plus whatever default styling the IDE applies).\n*/\n.mne-repr-table {\n    display: inline;  /* prevent using full container width */\n}\n.mne-repr-table tr.mne-repr-section-header > th {\n    padding-top: 1rem;\n    text-align: left;\n    vertical-align: middle;\n}\n.mne-repr-section-toggle > button {\n    all: unset;\n    display: block;\n    height: 1rem;\n    width: 1rem;\n}\n.mne-repr-section-toggle > button > svg {\n    height: 60%;\n}\n\n/* transition (rotation) effects on the collapser button */\n.mne-repr-section-toggle > button.collapsed > svg {\n    transition: 0.1s ease-out;\n    transform: rotate(-90deg);\n}\n.mne-repr-section-toggle > button:not(.collapsed) > svg {\n    transition: 0.1s ease-out;\n    transform: rotate(0deg);\n}\n\n/* hide collapsed table rows */\n.mne-repr-collapsed {\n    display: none;\n}\n\n\n@layer {\n    /*\n    Selectors in a `@layer` will always be lower-precedence than selectors outside the\n    layer. So even though e.g. `div.output_html` is present in the sphinx-rendered\n    website docs, the styles here won't take effect there as long as some other rule\n    somewhere in the page's CSS targets the same element.\n\n    In IDEs or Jupyter notebooks, though, the CSS files from the sphinx theme,\n    sphinx-gallery, and bootstrap are unavailable, so these styles will apply.\n\n    Notes:\n\n    - the selector `.accordion-body` is for MNE Reports\n    - the selector `.output_html` is for VSCode's notebook interface\n    - the selector `.jp-RenderedHTML` is for Jupyter notebook\n    - variables starting with `--theme-` are VSCode-specific.\n    - variables starting with `--jp-` are Jupyter styles, *some of which* are also\n      available in VSCode. Here we try the `--theme-` variable first, then fall back to\n      the `--jp-` ones.\n    */\n    .mne-repr-table {\n        --mne-toggle-color: var(--theme-foreground, var(--jp-ui-font-color1));\n        --mne-button-bg-color: var(--theme-button-background, var(--jp-info-color0, var(--jp-content-link-color)));\n        --mne-button-fg-color: var(--theme-button-foreground, var(--jp-ui-inverse-font-color0, var(--jp-editor-background)));\n        --mne-button-hover-bg-color: var(--theme-button-hover-background, var(--jp-info-color1));\n        --mne-button-radius: var(--jp-border-radius, 0.25rem);\n    }\n    /* chevron position/alignment; in VSCode it looks ok without adjusting */\n    .accordion-body .mne-repr-section-toggle > button,\n    .jp-RenderedHTML .mne-repr-section-toggle > button {\n        padding: 0 0 45% 25% !important;\n    }\n    /* chevron color; MNE Report doesn't have light/dark mode */\n    div.output_html .mne-repr-section-toggle > button > svg > path,\n    .jp-RenderedHTML .mne-repr-section-toggle > button > svg > path {\n        fill: var(--mne-toggle-color);\n    }\n    .accordion-body .mne-ch-names-btn,\n    div.output_html .mne-ch-names-btn,\n    .jp-RenderedHTML .mne-ch-names-btn {\n        -webkit-border-radius: var(--mne-button-radius);\n        -moz-border-radius: var(--mne-button-radius);\n        border-radius: var(--mne-button-radius);\n        border: none;\n        background-image: none;\n        background-color: var(--mne-button-bg-color);\n        color: var(--mne-button-fg-color);\n        font-size: inherit;\n        min-width: 1.5rem;\n        padding: 0.25rem;\n        text-align: center;\n        text-decoration: none;\n    }\n    .accordion-body .mne-ch-names-btn:hover,\n    div.output_html .mne.ch-names-btn:hover,\n    .jp-RenderedHTML .mne-ch-names-btn:hover {\n        background-color: var(--mne-button-hover-bg-color);\n        text-decoration: underline;\n    }\n    .accordion-body .mne-ch-names-btn:focus-visible,\n    div.output_html .mne-ch-names-btn:focus-visible,\n    .jp-RenderedHTML .mne-ch-names-btn:focus-visible {\n        outline: 0.1875rem solid var(--mne-button-bg-color) !important;\n        outline-offset: 0.1875rem !important;\n    }\n}\n</style>\n\n\n\n<table class=\"table mne-repr-table\">\n    \n\n\n\n\n\n\n\n<tr class=\"mne-repr-section-header general-c82ac63b-1d68-4561-8f35-ad716f25c083\"\n     title=\"Hide section\" \n    onclick=\"toggleVisibility('general-c82ac63b-1d68-4561-8f35-ad716f25c083')\">\n    <th class=\"mne-repr-section-toggle\">\n        <button >\n            <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 512 512\"><!--!Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free Copyright 2024 Fonticons, Inc.--><path d=\"M233.4 406.6c12.5 12.5 32.8 12.5 45.3 0l192-192c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L256 338.7 86.6 169.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3l192 192z\"/></svg>\n        </button>\n    </th>\n    <th colspan=\"2\">\n        <strong>General</strong>\n    </th>\n</tr>\n\n\n<tr class=\"repr-element general-c82ac63b-1d68-4561-8f35-ad716f25c083 \">\n    <td class=\"mne-repr-section-toggle\"></td>\n    <td>Filename(s)</td>\n    <td>\n        \n        sample_audvis_raw.fif\n        \n        \n    </td>\n</tr>\n\n<tr class=\"repr-element general-c82ac63b-1d68-4561-8f35-ad716f25c083 \">\n    <td class=\"mne-repr-section-toggle\"></td>\n    <td>MNE object type</td>\n    <td>Raw</td>\n</tr>\n<tr class=\"repr-element general-c82ac63b-1d68-4561-8f35-ad716f25c083 \">\n    <td class=\"mne-repr-section-toggle\"></td>\n    <td>Measurement date</td>\n    \n    <td>2002-12-03 at 19:01:10 UTC</td>\n    \n</tr>\n<tr class=\"repr-element general-c82ac63b-1d68-4561-8f35-ad716f25c083 \">\n    <td class=\"mne-repr-section-toggle\"></td>\n    <td>Participant</td>\n    \n    <td>Unknown</td>\n    \n</tr>\n<tr class=\"repr-element general-c82ac63b-1d68-4561-8f35-ad716f25c083 \">\n    <td class=\"mne-repr-section-toggle\"></td>\n    <td>Experimenter</td>\n    \n    <td>MEG</td>\n    \n</tr>\n    \n\n\n\n\n\n\n\n<tr class=\"mne-repr-section-header acquisition-15116e97-e841-45c8-b98d-0452de39d627\"\n     title=\"Hide section\" \n    onclick=\"toggleVisibility('acquisition-15116e97-e841-45c8-b98d-0452de39d627')\">\n    <th class=\"mne-repr-section-toggle\">\n        <button >\n            <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 512 512\"><!--!Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free Copyright 2024 Fonticons, Inc.--><path d=\"M233.4 406.6c12.5 12.5 32.8 12.5 45.3 0l192-192c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L256 338.7 86.6 169.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3l192 192z\"/></svg>\n        </button>\n    </th>\n    <th colspan=\"2\">\n        <strong>Acquisition</strong>\n    </th>\n</tr>\n\n\n<tr class=\"repr-element acquisition-15116e97-e841-45c8-b98d-0452de39d627 \">\n    <td class=\"mne-repr-section-toggle\"></td>\n    <td>Duration</td>\n    <td>00:04:38 (HH:MM:SS)</td>\n</tr>\n\n\n\n\n\n\n\n\n<tr class=\"repr-element acquisition-15116e97-e841-45c8-b98d-0452de39d627 \">\n    <td class=\"mne-repr-section-toggle\"></td>\n    <td>Sampling frequency</td>\n    <td>150.00 Hz</td>\n</tr>\n\n\n<tr class=\"repr-element acquisition-15116e97-e841-45c8-b98d-0452de39d627 \">\n    <td class=\"mne-repr-section-toggle\"></td>\n    <td>Time points</td>\n    <td>41,657</td>\n</tr>\n\n\n    \n\n\n\n\n\n\n\n<tr class=\"mne-repr-section-header channels-9eee3439-0775-4fe8-9f91-d02f08fd238f\"\n     title=\"Hide section\" \n    onclick=\"toggleVisibility('channels-9eee3439-0775-4fe8-9f91-d02f08fd238f')\">\n    <th class=\"mne-repr-section-toggle\">\n        <button >\n            <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 512 512\"><!--!Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free Copyright 2024 Fonticons, Inc.--><path d=\"M233.4 406.6c12.5 12.5 32.8 12.5 45.3 0l192-192c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L256 338.7 86.6 169.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3l192 192z\"/></svg>\n        </button>\n    </th>\n    <th colspan=\"2\">\n        <strong>Channels</strong>\n    </th>\n</tr>\n\n\n    \n<tr class=\"repr-element channels-9eee3439-0775-4fe8-9f91-d02f08fd238f \">\n    <td class=\"mne-repr-section-toggle\"></td>\n    <td>EEG</td>\n    <td>\n        <button class=\"mne-ch-names-btn sd-sphinx-override sd-btn sd-btn-info sd-text-wrap sd-shadow-sm\" onclick=\"alert('Good EEG:\\n\\nEEG&nbsp;001, EEG&nbsp;002, EEG&nbsp;003, EEG&nbsp;004, EEG&nbsp;005, EEG&nbsp;006, EEG&nbsp;007, EEG&nbsp;008, EEG&nbsp;009, EEG&nbsp;010, EEG&nbsp;011, EEG&nbsp;012, EEG&nbsp;013, EEG&nbsp;014, EEG&nbsp;015, EEG&nbsp;016, EEG&nbsp;017, EEG&nbsp;018, EEG&nbsp;019, EEG&nbsp;020, EEG&nbsp;021, EEG&nbsp;022, EEG&nbsp;023, EEG&nbsp;024, EEG&nbsp;025, EEG&nbsp;026, EEG&nbsp;027, EEG&nbsp;028, EEG&nbsp;029, EEG&nbsp;030, EEG&nbsp;031, EEG&nbsp;032, EEG&nbsp;033, EEG&nbsp;034, EEG&nbsp;035, EEG&nbsp;036, EEG&nbsp;037, EEG&nbsp;038, EEG&nbsp;039, EEG&nbsp;040, EEG&nbsp;041, EEG&nbsp;042, EEG&nbsp;043, EEG&nbsp;044, EEG&nbsp;045, EEG&nbsp;046, EEG&nbsp;047, EEG&nbsp;048, EEG&nbsp;049, EEG&nbsp;050, EEG&nbsp;051, EEG&nbsp;052, EEG&nbsp;054, EEG&nbsp;055, EEG&nbsp;056, EEG&nbsp;057, EEG&nbsp;058, EEG&nbsp;059, EEG&nbsp;060')\" title=\"(Click to open in popup)&#13;&#13;EEG&nbsp;001, EEG&nbsp;002, EEG&nbsp;003, EEG&nbsp;004, EEG&nbsp;005, EEG&nbsp;006, EEG&nbsp;007, EEG&nbsp;008, EEG&nbsp;009, EEG&nbsp;010, EEG&nbsp;011, EEG&nbsp;012, EEG&nbsp;013, EEG&nbsp;014, EEG&nbsp;015, EEG&nbsp;016, EEG&nbsp;017, EEG&nbsp;018, EEG&nbsp;019, EEG&nbsp;020, EEG&nbsp;021, EEG&nbsp;022, EEG&nbsp;023, EEG&nbsp;024, EEG&nbsp;025, EEG&nbsp;026, EEG&nbsp;027, EEG&nbsp;028, EEG&nbsp;029, EEG&nbsp;030, EEG&nbsp;031, EEG&nbsp;032, EEG&nbsp;033, EEG&nbsp;034, EEG&nbsp;035, EEG&nbsp;036, EEG&nbsp;037, EEG&nbsp;038, EEG&nbsp;039, EEG&nbsp;040, EEG&nbsp;041, EEG&nbsp;042, EEG&nbsp;043, EEG&nbsp;044, EEG&nbsp;045, EEG&nbsp;046, EEG&nbsp;047, EEG&nbsp;048, EEG&nbsp;049, EEG&nbsp;050, EEG&nbsp;051, EEG&nbsp;052, EEG&nbsp;054, EEG&nbsp;055, EEG&nbsp;056, EEG&nbsp;057, EEG&nbsp;058, EEG&nbsp;059, EEG&nbsp;060\">\n            59\n        </button>\n\n        \n        \n        and <button class=\"mne-ch-names-btn sd-sphinx-override sd-btn sd-btn-info sd-text-wrap sd-shadow-sm\" onclick=\"alert('Bad EEG:\\n\\nEEG&nbsp;053')\" title=\"(Click to open in popup)&#13;&#13;EEG&nbsp;053\">\n            1 bad\n        </button>\n        \n    </td>\n</tr>\n\n\n<tr class=\"repr-element channels-9eee3439-0775-4fe8-9f91-d02f08fd238f \">\n    <td class=\"mne-repr-section-toggle\"></td>\n    <td>Head & sensor digitization</td>\n    \n    <td>146 points</td>\n    \n</tr>\n    \n\n\n\n\n\n\n\n<tr class=\"mne-repr-section-header filters-4ddbe2f5-8bc6-4296-a464-d24738c23d50\"\n     title=\"Hide section\" \n    onclick=\"toggleVisibility('filters-4ddbe2f5-8bc6-4296-a464-d24738c23d50')\">\n    <th class=\"mne-repr-section-toggle\">\n        <button >\n            <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 512 512\"><!--!Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free Copyright 2024 Fonticons, Inc.--><path d=\"M233.4 406.6c12.5 12.5 32.8 12.5 45.3 0l192-192c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L256 338.7 86.6 169.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3l192 192z\"/></svg>\n        </button>\n    </th>\n    <th colspan=\"2\">\n        <strong>Filters</strong>\n    </th>\n</tr>\n\n\n<tr class=\"repr-element filters-4ddbe2f5-8bc6-4296-a464-d24738c23d50 \">\n    <td class=\"mne-repr-section-toggle\"></td>\n    <td>Highpass</td>\n    <td>0.10 Hz</td>\n</tr>\n\n\n<tr class=\"repr-element filters-4ddbe2f5-8bc6-4296-a464-d24738c23d50 \">\n    <td class=\"mne-repr-section-toggle\"></td>\n    <td>Lowpass</td>\n    <td>75.00 Hz</td>\n</tr>\n\n\n<tr class=\"repr-element filters-4ddbe2f5-8bc6-4296-a464-d24738c23d50 \">\n    <td class=\"mne-repr-section-toggle\"></td>\n    <td>Projections</td>\n    <td>\n        \n        Average EEG reference (on)\n        \n        \n    </td>\n</tr>\n\n</table>\n```\n:::\n:::\n\n\n# Epoching the data\nNow we can epoch the data which means rearranging it into **short segments** centered around the presented stimuli. The segment duration is defined by the parameters `tmin` and `tmax` that are passed to MNE's `Epochs` class.\nPer default, the signal before 0, the stimulus onset, is used as a **baseline** which means that it's average is subtracted from the rest of the epoch.\n\nI think this is **not a good default** choice because activity in the baseline period (e.g. due to anticipation of the stimulus) can be projected into the rest of the epoch and **create spurious features** that look like actual brain responses.\nIn most cases, baselining is **not necessary** if the data were detrended or high pass filtered.\n\nAfter epoching,we can average all segments to obtain the **event related potential** (ERP), which is the part of the brain response, evoked by the stimuli (since spontaneous activity will average out).\nWe can visualize the ERP to make sure that our preprocessing was effective and we have **clean data**.\n\n::: {#961ce55a .cell execution_count=9}\n``` {.python .cell-code}\nfrom mne.epochs import Epochs\n\nevent_id = {\"auditory/left\": 1, \"auditory/right\": 2}\nepochs = Epochs(raw, events, event_id, tmin=-0.1, tmax=0.4, baseline=None)\nepochs.average().plot();\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNot setting metadata\n145 matching events found\nNo baseline correction applied\nCreated an SSP operator (subspace dimension = 1)\n1 projection items activated\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![Event related potential (ERP) obtained by averaging the responses to all auditory stimuli. Each line represents one EEG channel.](index_files/figure-html/cell-10-output-2.png){width=626 height=299}\n:::\n:::\n\n\n# What next?\nWe removed offsets, drifts and power-line noise, re-referenced the data to a robust average and epoched them. \nNow the epochs may be **ready for statistical analysis** or it they may require more cleaning. In the next post on preprocessing I will explain how to **remove eye blink artifacts** and identify and **remove data segments** that are beyond saving.\n\n# Footnotes\n[^1]: Delorme, A. (2023). EEG is better left alone. Scientific reports, 13(1), 2372.\n\n[^2]: The preprocessing steps described here apply to MEG as well - I just omitted it for the sake of simplicity.\n\n[^3]:For a detailed investigation of this issue see de Cheveigné, A., & Nelken, I. (2019). Filters: when, why, and how (not) to use them. Neuron, 102(2), 280-293.\n\n[^4]:The detrending algorithm is described in: de Cheveigné, A., & Arzounian, D. (2018). Robust detrending, rereferencing, outlier detection, and inpainting for multichannel data. NeuroImage, 172, 903-912.\n\n[^5]: The denoising algorithm is described in: de Cheveigné, A. (2020). ZapLine: A simple and effective method to remove power line artifacts. NeuroImage, 207, 116356.\n\n[^6]: RANSAC is part of another EEG preprocessing pipeline described in: Bigdely-Shamlo, N., Mullen, T., Kothe, C., Su, K. M., & Robbins, K. A. (2015). The PREP pipeline: standardized preprocessing for large-scale EEG analysis. Frontiers in neuroinformatics, 9, 16.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdn.jsdelivr.net/npm/requirejs@2.3.6/require.min.js\" integrity=\"sha384-c9c+LnTbwQ3aujuU7ULEPVvgLs+Fn6fJUvIGTsuu1ZcCf11fiEubah0ttpca4ntM sha384-6V1/AdqZRWk1KAlWbKBlGhN7VG4iE/yAZcO6NZPMF8od0vukrvr0tg4qY6NSrItx\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js\" integrity=\"sha384-ZvpUoO/+PpLXR1lu4jmpXWu80pZlYUAfxl5NsBMWOEPSjUn/6Z/hRTt8+pR6L4N2\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n<script src=\"https://cdn.jsdelivr.net/npm/@jupyter-widgets/html-manager@*/dist/embed-amd.js\" crossorigin=\"anonymous\"></script>\n"
      ],
      "include-after-body": [
        "<script type=application/vnd.jupyter.widget-state+json>\n{\"state\":{\"2129c8b6e0a0437ba7ce8f403467fd4c\":{\"model_module\":\"@jupyter-widgets/base\",\"model_module_version\":\"2.0.0\",\"model_name\":\"LayoutModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/base\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"LayoutModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"LayoutView\",\"align_content\":null,\"align_items\":null,\"align_self\":null,\"border_bottom\":null,\"border_left\":null,\"border_right\":null,\"border_top\":null,\"bottom\":null,\"display\":null,\"flex\":null,\"flex_flow\":null,\"grid_area\":null,\"grid_auto_columns\":null,\"grid_auto_flow\":null,\"grid_auto_rows\":null,\"grid_column\":null,\"grid_gap\":null,\"grid_row\":null,\"grid_template_areas\":null,\"grid_template_columns\":null,\"grid_template_rows\":null,\"height\":null,\"justify_content\":null,\"justify_items\":null,\"left\":null,\"margin\":null,\"max_height\":null,\"max_width\":null,\"min_height\":null,\"min_width\":null,\"object_fit\":null,\"object_position\":null,\"order\":null,\"overflow\":null,\"padding\":null,\"right\":null,\"top\":null,\"visibility\":null,\"width\":null}},\"3cf54bf3bfd34cb8b691bf5982f7542a\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"2.0.0\",\"model_name\":\"HTMLModel\",\"state\":{\"_dom_classes\":[],\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"HTMLModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/controls\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"HTMLView\",\"description\":\"\",\"description_allow_html\":false,\"layout\":\"IPY_MODEL_2129c8b6e0a0437ba7ce8f403467fd4c\",\"placeholder\":\"​\",\"style\":\"IPY_MODEL_a75952e7a6c74b4c81cde8c601bee45a\",\"tabbable\":null,\"tooltip\":null,\"value\":\"100%\"}},\"4fb0b544a9f544ecbe5c857426ce83e4\":{\"model_module\":\"@jupyter-widgets/base\",\"model_module_version\":\"2.0.0\",\"model_name\":\"LayoutModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/base\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"LayoutModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"LayoutView\",\"align_content\":null,\"align_items\":null,\"align_self\":null,\"border_bottom\":null,\"border_left\":null,\"border_right\":null,\"border_top\":null,\"bottom\":null,\"display\":null,\"flex\":null,\"flex_flow\":null,\"grid_area\":null,\"grid_auto_columns\":null,\"grid_auto_flow\":null,\"grid_auto_rows\":null,\"grid_column\":null,\"grid_gap\":null,\"grid_row\":null,\"grid_template_areas\":null,\"grid_template_columns\":null,\"grid_template_rows\":null,\"height\":null,\"justify_content\":null,\"justify_items\":null,\"left\":null,\"margin\":null,\"max_height\":null,\"max_width\":null,\"min_height\":null,\"min_width\":null,\"object_fit\":null,\"object_position\":null,\"order\":null,\"overflow\":null,\"padding\":null,\"right\":null,\"top\":null,\"visibility\":null,\"width\":null}},\"853f0d1d7b9e4384aa278c30dc23c9fb\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"2.0.0\",\"model_name\":\"HTMLModel\",\"state\":{\"_dom_classes\":[],\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"HTMLModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/controls\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"HTMLView\",\"description\":\"\",\"description_allow_html\":false,\"layout\":\"IPY_MODEL_4fb0b544a9f544ecbe5c857426ce83e4\",\"placeholder\":\"​\",\"style\":\"IPY_MODEL_87fff34028814a66bf236f17e63a5e93\",\"tabbable\":null,\"tooltip\":null,\"value\":\"  : 55/55 [00:03&lt;00:00,   18.02it/s]\"}},\"87fff34028814a66bf236f17e63a5e93\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"2.0.0\",\"model_name\":\"HTMLStyleModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"HTMLStyleModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"StyleView\",\"background\":null,\"description_width\":\"\",\"font_size\":null,\"text_color\":null}},\"8cd70c731c9143549e3937883f82fe63\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"2.0.0\",\"model_name\":\"FloatProgressModel\",\"state\":{\"_dom_classes\":[],\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"FloatProgressModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/controls\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"ProgressView\",\"bar_style\":\"success\",\"description\":\"\",\"description_allow_html\":false,\"layout\":\"IPY_MODEL_cb309ef8cc2c40a49c0fe8b1ed71ade3\",\"max\":55,\"min\":0,\"orientation\":\"horizontal\",\"style\":\"IPY_MODEL_af2d149899b544c49108b0de3b58a12b\",\"tabbable\":null,\"tooltip\":null,\"value\":55}},\"a75952e7a6c74b4c81cde8c601bee45a\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"2.0.0\",\"model_name\":\"HTMLStyleModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"HTMLStyleModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"StyleView\",\"background\":null,\"description_width\":\"\",\"font_size\":null,\"text_color\":null}},\"af2d149899b544c49108b0de3b58a12b\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"2.0.0\",\"model_name\":\"ProgressStyleModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"ProgressStyleModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"StyleView\",\"bar_color\":null,\"description_width\":\"\"}},\"c8fadc893d7947de9fb364045c2c6d8b\":{\"model_module\":\"@jupyter-widgets/base\",\"model_module_version\":\"2.0.0\",\"model_name\":\"LayoutModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/base\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"LayoutModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"LayoutView\",\"align_content\":null,\"align_items\":null,\"align_self\":null,\"border_bottom\":null,\"border_left\":null,\"border_right\":null,\"border_top\":null,\"bottom\":null,\"display\":null,\"flex\":null,\"flex_flow\":null,\"grid_area\":null,\"grid_auto_columns\":null,\"grid_auto_flow\":null,\"grid_auto_rows\":null,\"grid_column\":null,\"grid_gap\":null,\"grid_row\":null,\"grid_template_areas\":null,\"grid_template_columns\":null,\"grid_template_rows\":null,\"height\":null,\"justify_content\":null,\"justify_items\":null,\"left\":null,\"margin\":null,\"max_height\":null,\"max_width\":null,\"min_height\":null,\"min_width\":null,\"object_fit\":null,\"object_position\":null,\"order\":null,\"overflow\":null,\"padding\":null,\"right\":null,\"top\":null,\"visibility\":null,\"width\":null}},\"cb309ef8cc2c40a49c0fe8b1ed71ade3\":{\"model_module\":\"@jupyter-widgets/base\",\"model_module_version\":\"2.0.0\",\"model_name\":\"LayoutModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/base\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"LayoutModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"LayoutView\",\"align_content\":null,\"align_items\":null,\"align_self\":null,\"border_bottom\":null,\"border_left\":null,\"border_right\":null,\"border_top\":null,\"bottom\":null,\"display\":null,\"flex\":null,\"flex_flow\":null,\"grid_area\":null,\"grid_auto_columns\":null,\"grid_auto_flow\":null,\"grid_auto_rows\":null,\"grid_column\":null,\"grid_gap\":null,\"grid_row\":null,\"grid_template_areas\":null,\"grid_template_columns\":null,\"grid_template_rows\":null,\"height\":null,\"justify_content\":null,\"justify_items\":null,\"left\":null,\"margin\":null,\"max_height\":null,\"max_width\":null,\"min_height\":null,\"min_width\":null,\"object_fit\":null,\"object_position\":null,\"order\":null,\"overflow\":null,\"padding\":null,\"right\":null,\"top\":null,\"visibility\":null,\"width\":null}},\"f946be2a914a45bd980337c1a921ac21\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"2.0.0\",\"model_name\":\"HBoxModel\",\"state\":{\"_dom_classes\":[],\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"HBoxModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/controls\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"HBoxView\",\"box_style\":\"\",\"children\":[\"IPY_MODEL_3cf54bf3bfd34cb8b691bf5982f7542a\",\"IPY_MODEL_8cd70c731c9143549e3937883f82fe63\",\"IPY_MODEL_853f0d1d7b9e4384aa278c30dc23c9fb\"],\"layout\":\"IPY_MODEL_c8fadc893d7947de9fb364045c2c6d8b\",\"tabbable\":null,\"tooltip\":null}}},\"version_major\":2,\"version_minor\":0}\n</script>\n"
      ]
    }
  }
}