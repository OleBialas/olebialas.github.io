{
  "hash": "41a3cec4162cee8abee078b1b4edfcdf",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"EEG preprocessing II: eye-artifacts, repairing and rejecting\"\ndescription: \"The second part of this series demonstrate additional preprocessing steps. Specifically, it addresses the problem of eye artifacts which are omnipresent in EEG recordings. It also demonstartes a procedure for repairing and rejecting noise-contaminated channels and segments. \"\ndate: \"2024-02-23\"\nimage: featured.png\nimage-alt: |\n  `matplotlib` plot of an event-related potential\naliases:\n  - /post/eeg-preprocessing-2/index.html\n  - /post/eeg-preprocessing-2.html\nengine: Jupyter\nformat:\n  html: default\n  ipynb: default\nfreeze: auto\n---\n\nThe [previous post on preprocessing EEG](posts/eeg_preprocessing) presented a minimally invasive pipeline of procedures that are necessary in most EEG analyses.  In this post I present additional steps that might be useful if the data is still not **sufficiently cleaned**. First, I will address **eye blinks** which is one of the most prevalent sources of artifacts in EEG recordings. After that, I'll demonstrate a method to repair or remove segments of the data **contaminated with noise**. \n\n# Prerequisites\n\nFirst, we need to install some packages that provide us with the equired preprocessing functions:\n```\npip install mne meegkit pyprep autoreject\n```\nThen, we have to dowload the sample data from MNE Python and clean it using the steps described [in the previous post on EEG preprocessing](posts/eeg_preprocessing).\nThe code below does exactly that --- if you want a more detailed explanation, read the original post.\nRunning the cell below produces the cleaned `epochs` to which we will apply further preprocessing, starting with the removal of **eye artifacts**.\n\n::: {#3abfabe1 .cell execution_count=1}\n``` {.python .cell-code code-fold=\"true\"}\nimport numpy as np\nfrom mne.io import read_raw_fif\nfrom mne.datasets.sample import data_path\nfrom mne import find_events\nfrom mne.epochs import Epochs\nfrom meegkit.detrend import detrend\nfrom meegkit.dss import dss_line\nfrom pyprep.ransac import find_bad_by_ransac\n\nraw = read_raw_fif(data_path() / \"MEG/sample/sample_audvis_raw.fif\")\nevents = find_events(raw)\nraw.pick(picks=\"eeg\")\nraw, events = raw.resample(150, events=events)\nX = raw.get_data().T  # transpose so the data is organized time-by-channels\nX, _, _ = detrend(X, order=1)\nX, _, _ = detrend(X, order=6)\nraw._data = X.T  # overwrite raw data\nX, noise = dss_line(X, fline=60, sfreq=raw.info[\"sfreq\"], nremove=3)\nraw._data = X.T\nbads, _ = find_bad_by_ransac(\n    data=raw.get_data(),\n    sample_rate=raw.info[\"sfreq\"],\n    complete_chn_labs=np.asarray(raw.info[\"ch_names\"]),\n    chn_pos=np.stack([ch[\"loc\"][0:3] for ch in raw.info[\"chs\"]]),\n    exclude=[],\n    corr_thresh=0.9,\n)\nraw_clean = raw.copy()\nraw_clean.info[\"bads\"] = bads\nraw_clean.interpolate_bads()\nraw_clean.set_eeg_reference(\"average\", projection=True)  # compute the reference\nraw.add_proj(raw_clean.info[\"projs\"][0])\ndel raw_clean  # delete the copy\nraw.apply_proj()  # apply the reference\nevent_id = {\"auditory/left\": 1, \"auditory/right\": 2}\nepochs = Epochs(raw, events, event_id, tmin=-0.1, tmax=0.4, baseline=None, preload=True)\n```\n:::\n\n\n# What are eye artifacts?\nWhile it is often assumed that eye artifacts are the result of muscle activity, they are actually the result of a **ionic gradient** in the retinal pigment epithelium that makes the eye an **electric dipole** [^1]. Thus, moving the eyes and the dipole **induces** a change in voltage picked up by the sensors that is roughly proportional to the **amplitude** of the movement. Because this could overshadow the neural responses, many studies eliminate eye movements by making participants **fixate** a point during the experiment.\n\nHowever, another kind of eye artifact may still occur - **blinks**. Eye blinks affect the measured voltage because the eye lid **changes the resistance** between the positively charged cornea and the forehead. Fortunately, these eye artifacts are largely **independent** of each other and the brain activity which makes them ideal candidates for independent component analysis (ICA) [^2].\n\n# Identifying eye-blink components with ICA\nICA is an algorithm that finds a rotation matrix to separate the sensor data into components that are **mutually independent** [^3].\nIn the code below, I fit an ICA to the epoched data. At maximum, ICA can capture as many components as there are channels. However, usually the data can be captured with **fewer components**. When the `n_components` parameter is set to a decimal number, the ICA will compute as many components as are necessary to explain this share of the total variance in the data. Because highpass filtering improves the quality of artifact separation [^4], I use a highpass filtered copy of the data for ICA.\n\n::: {#1ed3a83c .cell execution_count=2}\n``` {.python .cell-code}\nfrom matplotlib import pyplot as plt\nfrom mne.preprocessing import ICA\nfig, ax = plt.subplots(1, 5)\nica = ICA(n_components=0.99)\nica.fit(epochs.copy().filter(l_freq=2, h_freq=None))\nica.plot_components(range(5), axes=ax);\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSetting up high-pass filter at 2 Hz\n\nFIR filter parameters\n---------------------\nDesigning a one-pass, zero-phase, non-causal highpass filter:\n- Windowed time-domain design (firwin) method\n- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n- Lower passband edge: 2.00\n- Lower transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 1.00 Hz)\n- Filter length: 249 samples (1.660 s)\n\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nFitting ICA to data using 59 channels (please be patient, this may take a while)\n    Applying projection operator with 1 vector (pre-whitener computation)\n    Applying projection operator with 1 vector (pre-whitener application)\nSelecting by explained variance: 32 components\n    Applying projection operator with 1 vector (pre-whitener application)\nFitting ICA took 0.4s.\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![The first 5 inpedendent components found by ICA. These account for most of the variance in the data](index_files/figure-html/cell-3-output-3.png){width=540 height=137}\n:::\n:::\n\n\nThe components are ordered by **explained variance**, so the first few components have the largest impact on the signal.\nEach component is a **linear combination** of all channels and the weights indicate how much each channel affects that component [^5]. The first components depends almost solely on the **frontal channels** - a strong indicator that it represents eye-blink artifacts! \n\nAnother way to characterize the components is to obtain their **time course** by filtering the EEG signal using the component weights. The resulting time series is called the **component loading** and indicates the presence of that component in the data across time. In the code below, I compute the loading for all ICA components, select the first one and plot it after concatenating all epochs.\n\n::: {#16c5339f .cell execution_count=3}\n``` {.python .cell-code}\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\nsrc = ica.get_sources(epochs)\nsrc = src.get_data()[:, 0, :].flatten()\ntimes = np.linspace(0, len(src) / epochs.info[\"sfreq\"], len(src))\nplt.plot(times[:4000], src[:4000])\nplt.xlabel(\"Time [s]\")\nplt.ylabel(\"Component loading [a.u.]\");\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    Applying projection operator with 1 vector (pre-whitener application)\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![Loading of the components which indicates how active this component is throughout the recording](index_files/figure-html/cell-4-output-2.png){width=585 height=433}\n:::\n:::\n\n\nThe component loading is mostly flat except for **large amplitude spikes** - exactly what is expected from a signal that represents discrete eye blinks. After ensuring that the component captures blinks it can be removed from the data.\n\n# Automated component rejection\nOne could simply select and remove the eye blink component from the data. However, manual selection of components goes against the idea of an automated preprocessing pipeline. Instead, we can use the selected component as a **template** and classify new components as blinks by using the `corrmap` algorithm which selects components who's **correlation** with the template exceeds some **threshold** [^6]. To do this we can can store the blink component's index in the `ICA.labels_` attribute and save the ICA as template.\n\n::: {#2c543828 .cell execution_count=4}\n``` {.python .cell-code}\nimport tempfile\ntemp_dir = tempfile.TemporaryDirectory()\ntemplate = ica.copy()\ntemplate.labels_['blinks'] = [0]\ntemplate.save(temp_dir.name + '/template_ica.fif')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nWriting ICA solution to /tmp/tmp08xo0rwc/template_ica.fif...\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=4}\n```{=html}\n<table class=\"table mne-repr-table\">\n    <tr>\n        <th>Method</th>\n        <td>fastica</td>\n    </tr>\n    <tr>\n        <th>Fit parameters</th>\n        <td>algorithm=parallel<br />fun=logcosh<br />fun_args=None<br />max_iter=1000<br /></td>\n    </tr>\n    <tr>\n        <th>Fit</th>\n        <td>56 iterations on epochs (11020 samples)</td>\n    </tr>\n    \n    <tr>\n        <th>ICA components</th>\n        <td>32</td>\n    </tr>\n    <tr>\n        <th>Available PCA components</th>\n        <td>59</td>\n    </tr>\n    <tr>\n        <th>Channel types</th>\n        <td>eeg</td>\n    </tr>\n    <tr>\n        <th>ICA components marked for exclusion</th>\n        <td>&mdash;</td>\n    </tr>\n    \n</table>\n```\n:::\n:::\n\n\nNow we can iterate through all entries in the `.labels_` attribute and use `corrmap` to find components that are **similar** to the respective template. The first input for `corrmap` is the list of ICAs being processed. The second input is a tuple with the index of the ICA instance in the list and the component of that ICA being used as **template**. Similar components that are detected are stored in the `.labels_` attribute of the respective ICA instance.\n\n::: {#bcd08686 .cell execution_count=5}\n``` {.python .cell-code}\nfrom mne.preprocessing import read_ica, corrmap\ntemplate = read_ica(temp_dir.name + '/template_ica.fif')\n\nfor key, value in template.labels_.items():\n    corrmap([template, ica], (0, value[0]), label=key, threshold=0.85, plot=False)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nReading /tmp/tmp08xo0rwc/template_ica.fif ...\n    Read a total of 1 projection items:\n        Average EEG reference (1 x 60) active\nNow restoring ICA solution ...\nReady.\nMedian correlation with constructed map: 1.000\nAt least 1 IC detected for each subject.\n```\n:::\n:::\n\n\nOf course, this example is completely circular because we applied `corrmap` to the same data we used for selecting the template in the first place. However, once selected, the same template can be applied to **multiple recorings** and even **across experiments**, given that the electrode layout is the same. After all artifact components have been identified, we can exclude them when **applying** the ICA to the sensor data.\n\n::: {#0b2a7496 .cell execution_count=6}\n``` {.python .cell-code}\nbad_components = [value[0] for value in ica.labels_.values()]\nepochs.load_data() # make sure data is loaded\nepochs = ica.apply(epochs, exclude=bad_components)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nApplying ICA to Epochs instance\n    Applying projection operator with 1 vector (pre-whitener application)\n    Transforming to ICA space (32 components)\n    Zeroing out 1 ICA component\n    Projecting back using 59 PCA components\n```\n:::\n:::\n\n\n# When data must be rejected\nEven with all the preprocessing steps discussed in this guide, some data can't be saved.\nSometimes, a channels **loses contact** with the scalp or a segment is noise-ridden, for example due to **excessive movement**. In those cases, we have to remove that data so it won't **contaminate** the average response. Traditionally, EEG data is **manually inspected**, bad channels are interpolated and bad segments are annotated for rejection by hand. This is suboptimal for several reasons: first, scanning tens of hours of EEG recordings is tedious, **time consuming** and unfeasible for very large data sets. What's more, the manual approach **reduces reproducibility** because the criteria for what counts as a bad channel or segment are subjective. Finally, it is often not necessary to interpolate a channel for the entire recording if it is bad for **only a fraction**.\n\n# Introducing autoreject\nAll of these problems are addressed by the `autoreject` algorithm [^7], which is a procedure to identify and either **repair or reject** bad data segments. For each channel p, it estimates a peak-to-peak **threshold** &tau;. Each channel marks epochs as bad that exceeds their respective threshold. A trial is rejected if a **fraction &kappa;** of all channels marks it as bad. If less than &kappa; channels are bad, up to **&rho; are interpolated** to repair the epoch. All parameters, &tau; &kappa; and &rho; are **estimated from the data** using cross-validation. Thus the optimal set of parameters are those that **minimize the difference** between testing and training data. In this sense, `autoreject` acts similar to a **human observer** identifying outliers in the data. After installing the module with `pip install autoreject`, we can simply apply it to the epoched data. I also plot the **rejection log** to visualize the effect of `autoreject` on the data.\n\n::: {#a9608083 .cell execution_count=7}\n``` {.python .cell-code}\nfrom autoreject import AutoReject\n\nfig, ax = plt.subplots()\nar = AutoReject(verbose=False)\nepochs, log = ar.fit_transform(epochs, return_log=True)\nlog.plot(orientation=\"horizontal\", ax=ax);\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDropped 2 epochs: 32, 40\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![Matrix displaying the results of `autoreject`. Each column is one trial, and blue squares indicate interpolated sensors. Red columns were deemed \"bad\" and marked for rejection.](index_files/figure-html/cell-8-output-2.png){width=662 height=353}\n:::\n:::\n\n\nIn the plot below, blue marks channels that have been **interpolated** within a given epoch. Red marks channels that have been deemed bad but not interpolated because the number of bad channels **exceeded &rho;**. The red column at epoch 40 indicates that this epoch has been **rejected** because the number of bad channels **exceeded &kappa;**.\n\n\n# Conclusion\nThe repertoire of preprocessing methods outlined in this and the previous post is sufficient to clean data for most EEG projects. Importantly, all steps can be assembled into a **fully automated** pipeline. In the next and final post in this series, I will share a such a pipeline and demonstrate a method for estimating the **effectiveness** of each step.\n\n# Footnotes\n[^1]: This is referred to as the corneo-retinal dipole. A explanation of the underlying physiology can be found in: *Arden, G. B., & Constable, P. A. (2006). The electro-oculogram. Progress in retinal and eye research, 25(2), 207-248.*\n\n[^2]: A detailed investigation of eye-artifacts and their detection via ICA can be found in: *Plöchl, M., Ossandón, J. P., & König, P. (2012). Combining EEG and eye tracking: identification, characterization, and correction of eye movement artifacts in electroencephalographic data. Frontiers in human neuroscience, 6, 278.*\n\n[^3]: An in-depth explanation of ICA is beyond the scope of this post but can be found in: *Makeig, S., Bell, A., Jung, T. P., & Sejnowski, T. J. (1995). Independent component analysis of electroencephalographic data. Advances in neural information processing systems, 8.*\n\n[^4]: A highpass between 1 and 2 Hz before ICA is optimal, see *Winkler, I., Debener, S., Müller, K. R., & Tangermann, M. (2015, August). On the influence of high-pass filtering on ICA-based artifact reduction in EEG-ERP. In 2015 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC) (pp. 4101-4105). IEEE.*\n\n[^5]: The absolute sign of the component is meaningless and may change when ICA is performed repeatedly.\n\n[^6]: A detailed description of the corrmap algorithm can be found in *Viola, F. C., Thorne, J., Edmonds, B., Schneider, T., Eichele, T., & Debener, S. (2009). Semi-automatic identification of independent components representing EEG artifact. Clinical Neurophysiology, 120(5), 868-877.*\n\n[^7]: A detailed description of the autoreject algorithm can be found in *Jas, M., Engemann, D. A., Bekhti, Y., Raimondo, F., & Gramfort, A. (2017). Autoreject: Automated artifact rejection for MEG and EEG data. NeuroImage, 159, 417-429.*\n\n",
    "supporting": [
      "index_files/figure-html"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdn.jsdelivr.net/npm/requirejs@2.3.6/require.min.js\" integrity=\"sha384-c9c+LnTbwQ3aujuU7ULEPVvgLs+Fn6fJUvIGTsuu1ZcCf11fiEubah0ttpca4ntM sha384-6V1/AdqZRWk1KAlWbKBlGhN7VG4iE/yAZcO6NZPMF8od0vukrvr0tg4qY6NSrItx\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js\" integrity=\"sha384-ZvpUoO/+PpLXR1lu4jmpXWu80pZlYUAfxl5NsBMWOEPSjUn/6Z/hRTt8+pR6L4N2\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n<script src=\"https://cdn.jsdelivr.net/npm/@jupyter-widgets/html-manager@*/dist/embed-amd.js\" crossorigin=\"anonymous\"></script>\n"
      ],
      "include-after-body": [
        "<script type=application/vnd.jupyter.widget-state+json>\n{\"state\":{\"2060e6a100194c7583c4b67e5d4e37e5\":{\"model_module\":\"@jupyter-widgets/base\",\"model_module_version\":\"2.0.0\",\"model_name\":\"LayoutModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/base\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"LayoutModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"LayoutView\",\"align_content\":null,\"align_items\":null,\"align_self\":null,\"border_bottom\":null,\"border_left\":null,\"border_right\":null,\"border_top\":null,\"bottom\":null,\"display\":null,\"flex\":null,\"flex_flow\":null,\"grid_area\":null,\"grid_auto_columns\":null,\"grid_auto_flow\":null,\"grid_auto_rows\":null,\"grid_column\":null,\"grid_gap\":null,\"grid_row\":null,\"grid_template_areas\":null,\"grid_template_columns\":null,\"grid_template_rows\":null,\"height\":null,\"justify_content\":null,\"justify_items\":null,\"left\":null,\"margin\":null,\"max_height\":null,\"max_width\":null,\"min_height\":null,\"min_width\":null,\"object_fit\":null,\"object_position\":null,\"order\":null,\"overflow\":null,\"padding\":null,\"right\":null,\"top\":null,\"visibility\":null,\"width\":null}},\"332ff5bee5614618a0ecbad7aabf632b\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"2.0.0\",\"model_name\":\"HTMLModel\",\"state\":{\"_dom_classes\":[],\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"HTMLModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/controls\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"HTMLView\",\"description\":\"\",\"description_allow_html\":false,\"layout\":\"IPY_MODEL_99ede392924c4e7d8d26f0359ebf8e50\",\"placeholder\":\"​\",\"style\":\"IPY_MODEL_5dd214d652534bd8a1b40452eaaaa51a\",\"tabbable\":null,\"tooltip\":null,\"value\":\"  : 55/55 [00:02&lt;00:00,   20.13it/s]\"}},\"5cadc4da04df4f7d9c8907491168801e\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"2.0.0\",\"model_name\":\"FloatProgressModel\",\"state\":{\"_dom_classes\":[],\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"FloatProgressModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/controls\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"ProgressView\",\"bar_style\":\"success\",\"description\":\"\",\"description_allow_html\":false,\"layout\":\"IPY_MODEL_d131056aa7cb45f5aa5541aec4b83e49\",\"max\":55,\"min\":0,\"orientation\":\"horizontal\",\"style\":\"IPY_MODEL_e69f68088d824790a0cd2beefa25f852\",\"tabbable\":null,\"tooltip\":null,\"value\":55}},\"5dd214d652534bd8a1b40452eaaaa51a\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"2.0.0\",\"model_name\":\"HTMLStyleModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"HTMLStyleModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"StyleView\",\"background\":null,\"description_width\":\"\",\"font_size\":null,\"text_color\":null}},\"81dcc426415e491f89333c7f3517cdaa\":{\"model_module\":\"@jupyter-widgets/base\",\"model_module_version\":\"2.0.0\",\"model_name\":\"LayoutModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/base\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"LayoutModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"LayoutView\",\"align_content\":null,\"align_items\":null,\"align_self\":null,\"border_bottom\":null,\"border_left\":null,\"border_right\":null,\"border_top\":null,\"bottom\":null,\"display\":null,\"flex\":null,\"flex_flow\":null,\"grid_area\":null,\"grid_auto_columns\":null,\"grid_auto_flow\":null,\"grid_auto_rows\":null,\"grid_column\":null,\"grid_gap\":null,\"grid_row\":null,\"grid_template_areas\":null,\"grid_template_columns\":null,\"grid_template_rows\":null,\"height\":null,\"justify_content\":null,\"justify_items\":null,\"left\":null,\"margin\":null,\"max_height\":null,\"max_width\":null,\"min_height\":null,\"min_width\":null,\"object_fit\":null,\"object_position\":null,\"order\":null,\"overflow\":null,\"padding\":null,\"right\":null,\"top\":null,\"visibility\":null,\"width\":null}},\"8961c22178f04c7da5818cb1be6e089e\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"2.0.0\",\"model_name\":\"HTMLModel\",\"state\":{\"_dom_classes\":[],\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"HTMLModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/controls\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"HTMLView\",\"description\":\"\",\"description_allow_html\":false,\"layout\":\"IPY_MODEL_81dcc426415e491f89333c7f3517cdaa\",\"placeholder\":\"​\",\"style\":\"IPY_MODEL_ec12c91157664e96b1b4f6ba9e3f0169\",\"tabbable\":null,\"tooltip\":null,\"value\":\"100%\"}},\"99ede392924c4e7d8d26f0359ebf8e50\":{\"model_module\":\"@jupyter-widgets/base\",\"model_module_version\":\"2.0.0\",\"model_name\":\"LayoutModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/base\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"LayoutModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"LayoutView\",\"align_content\":null,\"align_items\":null,\"align_self\":null,\"border_bottom\":null,\"border_left\":null,\"border_right\":null,\"border_top\":null,\"bottom\":null,\"display\":null,\"flex\":null,\"flex_flow\":null,\"grid_area\":null,\"grid_auto_columns\":null,\"grid_auto_flow\":null,\"grid_auto_rows\":null,\"grid_column\":null,\"grid_gap\":null,\"grid_row\":null,\"grid_template_areas\":null,\"grid_template_columns\":null,\"grid_template_rows\":null,\"height\":null,\"justify_content\":null,\"justify_items\":null,\"left\":null,\"margin\":null,\"max_height\":null,\"max_width\":null,\"min_height\":null,\"min_width\":null,\"object_fit\":null,\"object_position\":null,\"order\":null,\"overflow\":null,\"padding\":null,\"right\":null,\"top\":null,\"visibility\":null,\"width\":null}},\"cd17e4fdccd94e5fb0d27b05d377e17b\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"2.0.0\",\"model_name\":\"HBoxModel\",\"state\":{\"_dom_classes\":[],\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"HBoxModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/controls\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"HBoxView\",\"box_style\":\"\",\"children\":[\"IPY_MODEL_8961c22178f04c7da5818cb1be6e089e\",\"IPY_MODEL_5cadc4da04df4f7d9c8907491168801e\",\"IPY_MODEL_332ff5bee5614618a0ecbad7aabf632b\"],\"layout\":\"IPY_MODEL_2060e6a100194c7583c4b67e5d4e37e5\",\"tabbable\":null,\"tooltip\":null}},\"d131056aa7cb45f5aa5541aec4b83e49\":{\"model_module\":\"@jupyter-widgets/base\",\"model_module_version\":\"2.0.0\",\"model_name\":\"LayoutModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/base\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"LayoutModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"LayoutView\",\"align_content\":null,\"align_items\":null,\"align_self\":null,\"border_bottom\":null,\"border_left\":null,\"border_right\":null,\"border_top\":null,\"bottom\":null,\"display\":null,\"flex\":null,\"flex_flow\":null,\"grid_area\":null,\"grid_auto_columns\":null,\"grid_auto_flow\":null,\"grid_auto_rows\":null,\"grid_column\":null,\"grid_gap\":null,\"grid_row\":null,\"grid_template_areas\":null,\"grid_template_columns\":null,\"grid_template_rows\":null,\"height\":null,\"justify_content\":null,\"justify_items\":null,\"left\":null,\"margin\":null,\"max_height\":null,\"max_width\":null,\"min_height\":null,\"min_width\":null,\"object_fit\":null,\"object_position\":null,\"order\":null,\"overflow\":null,\"padding\":null,\"right\":null,\"top\":null,\"visibility\":null,\"width\":null}},\"e69f68088d824790a0cd2beefa25f852\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"2.0.0\",\"model_name\":\"ProgressStyleModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"ProgressStyleModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"StyleView\",\"bar_color\":null,\"description_width\":\"\"}},\"ec12c91157664e96b1b4f6ba9e3f0169\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"2.0.0\",\"model_name\":\"HTMLStyleModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"HTMLStyleModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"StyleView\",\"background\":null,\"description_width\":\"\",\"font_size\":null,\"text_color\":null}}},\"version_major\":2,\"version_minor\":0}\n</script>\n"
      ]
    }
  }
}