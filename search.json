[
  {
    "objectID": "drafts/config_validation_with_pydantic/index.html",
    "href": "drafts/config_validation_with_pydantic/index.html",
    "title": "Automated Validation of Experiment Configuration using Pydantic",
    "section": "",
    "text": "The design of an experiment involves many choices: Which stimuli are displayed? How often are they repeated? What kind of responses are obtained? And so on ‚Ä¶ It is good practice to store these parameters in a configuration, separate from the experimental code. This allows us to store all parameters in one place and reconfigure the experiment without altering the code. However, that means that the configuration file carries a lot of responsibility - any error in that file could crash the experiment. What‚Äôs more, because Python is a dynamically typed language, the parameters stored in the configuration file are only evaluated once they are accessed. This means our experiment could start fine despite one parameter being invalid and then crash the first time that parameter is accessed. This is obviously not ideal - if our experimental configuration is invalid, we would like to know that before running the experiment. In this blog post I‚Äôll show you how to use type-hinting and a library called Pydantic to create robust experiments that are automatically validated."
  },
  {
    "objectID": "drafts/config_validation_with_pydantic/index.html#prerequisites",
    "href": "drafts/config_validation_with_pydantic/index.html#prerequisites",
    "title": "Automated Validation of Experiment Configuration using Pydantic",
    "section": "Prerequisites",
    "text": "Prerequisites\nTo implement an example experiment, we‚Äôll use a psychoacoustics library called slab (soundlab) 1. To reproduce the examples, you have to install both slab and pydantic:\npip install slab pydantic\nDepending on your operating system, additional steps may be required play sounds and record responses - check out slab‚Äôs documentation"
  },
  {
    "objectID": "drafts/config_validation_with_pydantic/index.html#configuring-an-experiment",
    "href": "drafts/config_validation_with_pydantic/index.html#configuring-an-experiment",
    "title": "Automated Validation of Experiment Configuration using Pydantic",
    "section": "Configuring an experiment",
    "text": "Configuring an experiment\nAs an example we‚Äôll implement a pure tone audiogram that uses an adaptive staircase to identify the hearing threshold. The following code presents a pure tone and then waits for the listener‚Äôs response (‚Äú1‚Äù if the sound was heard, ‚Äú2‚Äù if not). The intensity is decreased until the sound can not be heard, then increased again until it can, then decreased again and so on, until the direction has reversed 10 times. The threshold is obtained by averaging across all reversal points 2.\nimport slab\nstimulus = slab.Sound.tone(frequency=2000, duration=0.5)\nstimulus = stimulus.ramp(when=\"both\", duration=0.01)\nstairs = slab.Staircase(start_val=50, n_reversals=10)\nfor level in stairs:\n    stimulus.level = level\n    stairs.present_tone_trial(stimulus, key_codes=(ord(\"1\"), ord(\"2\")))\nThis works but any time we‚Äôll have to change the code every time we want to reconfigure the experiment. To avoid this, we can store all of parameters in one file, let‚Äôs call it config.json 3. For the audiogram, the content of this file may look like this:\n{\n    \"frequency\": 500,\n    \"stimulus_duration\": 0.5,\n    \"ramp_duration\": 0.01,\n    \"start_val\": 50,\n    \"n_reversals\": 10,\n    \"keys\": {\"yes\":\"1\", \"no\":\"2\"}\n}\nNow, we can load the parameters from config.json, and pass them to the audiogram. Let‚Äôs implement the loading of the config file and execution of the audiogram in two separate functions. We‚Äôll also add a little command line interface using the argparse module that allows us to run our experiment from the terminal. If we store the code below in a file called audiogram.py we can run the experiment from the command line by typing python audiogram.py config.json.\n\nfrom argparse import ArgumentParser\nimport json\nimport slab\n\n\ndef audiogram(cfg):\n    stimulus = slab.Sound.tone(cfg[\"frequency\"], cfg[\"stimulus_duration\"])\n    stimulus = stimulus.ramp(when=\"both\", duration=cfg[\"ramp_duration\"])\n    stairs = slab.Staircase(cfg[\"start_val\"], cfg[\"n_reversals\"])\n    for level in stairs:\n        stimulus.level = level\n        stairs.present_tone_trial(\n            stimulus, key_codes=(ord(cfg[\"keys\"][\"yes\"]), ord(cfg[\"keys\"][\"no\"]))\n        )\n    return stairs.threshold()\n\n\ndef load_config(config_file):\n    with open(config_file) as f:\n        cfg = json.load(f)\n    return cfg\n\n\ndef main(config_file):\n    cfg = load_config(config_file)\n    threshold = audiogram(cfg)\n    print(f\"The hearing threshold at {cfg[\"frequency\"]} Hz is {threshold} dB\")\n\n\nif __name__ == \"__main__\":\n    parser = ArgumentParser()\n    parser.add_argument(\"config_file\", type=str)\n    args = parser.parse_args()\n    main(args.config_file)\nThe ArgumentParser takes in the path to our config and passes it to the main() function. In turn, main() calls load_config() and passes the parameters to audiogram() which runs the actual experiment. While this does the same as our previous code, it allows us to reconfigure the experiment by merely editing config.json. Now, we can run audiograms for different frequencies and durations without ever touching our code!"
  },
  {
    "objectID": "drafts/config_validation_with_pydantic/index.html#making-sure-the-configuration-is-valid",
    "href": "drafts/config_validation_with_pydantic/index.html#making-sure-the-configuration-is-valid",
    "title": "Automated Validation of Experiment Configuration using Pydantic",
    "section": "Making sure the configuration is valid",
    "text": "Making sure the configuration is valid\nWhile the approach above may seem convenient, it carries large risks. The program has no knowledge of the stored parameters before they are accessed the first time. This means that the program may start despite an invalid parameters and then crash in the middle of the experiment when that parameter is first accessed. To avoid this, we could add a function that validates our configuration after it has been loaded, for example:\n   def load_config(config_file):\n    with open(config_file) as f:\n        cfg = json.load(f)\n    return validate_config(cfg)\n\n\ndef validate_config(cfg):\n    for key in [\n        \"frequency\",\n        \"stimulus_duration\",\n        \"ramp_duration\",\n        \"start_val\",\n        \"n_reversals\",\n        \"keys\",\n    ]:\n        assert key in cfg.keys()\n    assert isinstance(cfg[\"stimulus_duration\"], float)\n    assert isinstance(cfg[\"ramp_duration\"], float)\n    # ... more asserts\n    return cfg\nThe validate_config() function checks if the config contains all required keys and asserts that the duration parameters are floating numbers (slab interprets floating numbers as seconds and integers as samples). By stacking multiple assert statements we can test all conditions that define a valid config. However, this quickly makes the code unreadable. Whats more, the validation function does not fix the opacity issue. The program still does not know the content of the config before accessing it - if we forget important checks during validation, the experiment may still crash."
  },
  {
    "objectID": "drafts/config_validation_with_pydantic/index.html#pydantic-to-the-rescue",
    "href": "drafts/config_validation_with_pydantic/index.html#pydantic-to-the-rescue",
    "title": "Automated Validation of Experiment Configuration using Pydantic",
    "section": "Pydantic to the rescue",
    "text": "Pydantic to the rescue\nInstead of a validation function, we can use a Pydantic model. A model is defined as a class by inheriting from Pydantic‚Äôs BaseModel. The model is simply a collection of fields, each with a specific type.\nfrom Pydantic import BaseModel\n\nclass Config(BaseModel):\n    frequency: int\n    stimulus_duration: float\n    ramp_duration: float\n    start_val: int\n    n_reversal: int\n    keys: dict\nNow, after loading the config.json, we can simply create an instance of our Config model by passing the keyword arguments from the dictionary. While creating the instance, Pydantic will make sure all parameters are present, check if they have the correct type, try to convert them if they don‚Äôt and raise an error if conversion is not possible.\ndef load_config(config_file):\n    with open('config.json') as f:\n        config = json.load(f)\n    return Config(**config) \nNow our audiogram() function can get it‚Äôs parameters by accessing the fields of the config model:\ndef audiogram(cfg: Config) -&gt; float:\n    stimulus = slab.Sound.tone(cfg.frequency, cfg.stimulus_duration)\n    stimulus = stimulus.ramp(when=\"both\", duration=cfg.ramp_duration)\n    stairs = slab.Staircase(cfg.start_val, cfg.n_reversals)\n    for level in stairs:\n        stimulus.level = level\n        stairs.present_tone_trial(\n            stimulus, key_codes=(ord(cfg.keys[\"yes\"]), ord(cfg.keys[\"no\"]))\n        )\n    return stairs.threshold()"
  },
  {
    "objectID": "drafts/config_validation_with_pydantic/index.html#type-hinting",
    "href": "drafts/config_validation_with_pydantic/index.html#type-hinting",
    "title": "Automated Validation of Experiment Configuration using Pydantic",
    "section": "Type hinting",
    "text": "Type hinting\nYou may have noticed the new notation is the function definition above: def audiogram(cfg:Config) -&gt; float. These are type hints indicating that the audiogram function takes an argument that is an instance of the Config class and returns a floating number. These type hints allow tools like LSPs 4 and type-checkers like MyPy 5 to automatically check that the operations performed on any variable are valid given that variables type. This enables us to catch logical inconsistencies in our code before they affect the experiment!"
  },
  {
    "objectID": "drafts/config_validation_with_pydantic/index.html#extended-validation",
    "href": "drafts/config_validation_with_pydantic/index.html#extended-validation",
    "title": "Automated Validation of Experiment Configuration using Pydantic",
    "section": "Extended validation",
    "text": "Extended validation\nSometimes, we want to validate our data beyond making sure they are of the correct type. For example, in the audiogram, we may want to make sure that the toe frequency is within the human hearing range. For this purpose, Pydantic offers a field_validator decorator 6 that allows us to define validation functions that are automatically executed when a value is assigned to a specific field of the model. Below, we add a validation function that takes in the value assigned to the field frequency and makes sure it lies within the human hearing range.\nclass Config(BaseModel):\n    frequency: int\n    # other fields \n    \n    @field_validator(\"frequency\")\n    @staticmethod\n    def frequency_is_audible(value):\n        assert 20 &lt;= value &lt;= 20000\n        return value\nYou may have noticed that the keys field in the Config model is a dictionary. This creates the same problem we had in the beginning, namely that the content of the dictionary can not be validated before it is accessed. To overcome this, we can create another Pydantic model called Keys and require that the keys parameter in the Config model is an instance of that class.\n\nclass Config(BaseModel):\n    # other fields\n    keys: Keys\n\nclass Keys(BaseModel):\n    yes: str\n    no: str\n    \n    @field_validator(\"yes\", \"no\")\n    @staticmethod\n    def is_single_digit(value):\n        assert len(value)==1\n        return value\nBy nesting models within models, we can create detailed validation schemes that can handle even the most complex data!"
  },
  {
    "objectID": "drafts/config_validation_with_pydantic/index.html#closing-remarks",
    "href": "drafts/config_validation_with_pydantic/index.html#closing-remarks",
    "title": "Automated Validation of Experiment Configuration using Pydantic",
    "section": "Closing remarks",
    "text": "Closing remarks\nIn this post, we explored how to use Pydantic and type hints to create more robust experiments. You may wonder if all of this is necessary - after all, this was a rather simple example. However, the example was deliberately simplified to fit within the scope of a blog post and omitted key parts of the experiments such as writing response data. The more complex an experiment becomes, the more opportunities for errors there are. This is where type hinting can really shine. By using type hints and validation, we can make it impossible to represent states that our program can‚Äôt handle. This way, we don‚Äôt have to be mindful of all the parameters and operations carried out on them, our program will do it for us!\nThere is also no requirement to type hint and validate every last parameter of your experiment. In our example, we may just define the Config class, without any additional validator functions or nested models, and add a single type hint to the audiogram function. This is very little effort and substantially improves the robustness of the program. However, the tools demonstrated here also allow the definition of detailed validation schemes that can handle even the most complex data - it is your choice how to use them!\nIf you are interested in exploring the topics introduced here further, I can recommend checking out Pydantic‚Äôs documentatio as well as the book Robust Python by Patrick Viafore. Finally, here is the code for the complete type-hinted and validated experiment:\n``from argparse import ArgumentParser\nimport json\nimport slab\nfrom pydantic import BaseModel, field_validator\n\n\nclass Keys(BaseModel):\n    yes: str\n    no: str\n\n    @field_validator(\"yes\", \"no\")\n    @staticmethod\n    def is_single_digit(value):\n        assert len(value) == 1\n        return value\n\n\nclass Config(BaseModel):\n    frequency: int\n    stimulus_duration: float\n    ramp_duration: float\n    start_val: int\n    n_reversals: int\n    keys: Keys\n\n    @field_validator(\"frequency\")\n    @staticmethod\n    def frequency_is_audible(value):\n        assert 20 &lt;= value &lt;= 20000\n        return value\n\n\ndef audiogram(cfg: Config) -&gt; float:\n    stimulus = slab.Sound.tone(cfg.frequency, cfg.stimulus_duration)\n    stimulus = stimulus.ramp(when=\"both\", duration=cfg.ramp_duration)\n    stairs = slab.Staircase(cfg.start_val, cfg.n_reversals)\n    for level in stairs:\n        stimulus.level = level\n        stairs.present_tone_trial(\n            stimulus, key_codes=(ord(cfg.keys.yes), ord(cfg.keys.no))\n        )\n    return stairs.threshold()\n\n\ndef load_config(config_file: str) -&gt; Config:\n    with open(config_file) as f:\n        config = json.load(f)\n    return Config(**config)\n\n\ndef main(config_file: str):\n    config = load_config(config_file)\n    threshold = audiogram(config)\n    print(f\"The hearing threshold at {config.frequency} Hz is {threshold} dB\")\n\n\nif __name__ == \"__main__\":\n    parser = ArgumentParser()\n    parser.add_argument(\"config_file\", type=str)\n    args = parser.parse_args()\n    main(args.config_file)"
  },
  {
    "objectID": "drafts/config_validation_with_pydantic/index.html#footnotes",
    "href": "drafts/config_validation_with_pydantic/index.html#footnotes",
    "title": "Automated Validation of Experiment Configuration using Pydantic",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSch√∂nwiesner, M., & Bialas, O. (2021). s (ound) lab: An easy to learn Python package for designing and running psychoacoustic experiments. Journal of Open Source Software, 6(62), 3284.‚Ü©Ô∏é\nIf the setup is not calibrated, the hearing threshold will represent the absolute sound level but the intensity relative to other sounds.‚Ü©Ô∏é\nJSON stands for ‚ÄúJavaScript Object Notation‚Äù which is a widely used file standard.‚Ü©Ô∏é\nLSP stands for ‚ÄúLanguage Server Protocol‚Äù which is a program that runs in the background and provides your code editor with features like syntax highlighting and code completion.‚Ü©Ô∏é\nWhile MyPy is not covered in this post, it is a power tool for automated type checking, see the online documentation‚Ü©Ô∏é\nIn a nutshell, a decorator is a function that modifies the execution of another function. For more detail, see this post‚Ü©Ô∏é"
  },
  {
    "objectID": "talks.html",
    "href": "talks.html",
    "title": "Talks",
    "section": "",
    "text": "Managing Scientific Data and Workflows with DataLad\n\n\nAn introdury tutorial to the DataLad software for scientific data management.\n\n\n\n\n\nWednesday, the 20th of August, 2025\n\n\nOle Bialas, Micha≈Ç Szczepanik\n\n\n\n\n\n\n\n\n\n\n\n\nZwischen Lab und Laptop - Technische Herausforderungen der Modernen Neurowissenschaft\n\n\nA talk about the technical challenges neuroscientists face today.\n\n\n\n\n\nSunday, the 19th of January, 2025\n\n\nOle Bialas\n\n\n\n\n\n\n\n\n\n\n\n\nStandardizing and Sharing EEG Data\n\n\nAn introduction to the Brain Imaging Data Structure (BIDS) and the OpenNeuro platform.\n\n\n\n\n\nMonday, the 8th of April, 2024\n\n\nOle Bialas\n\n\n\n\n\n\n\n\n\n\n\n\nFundamentals of TRF-analyses in Python\n\n\nA demo of the mTRFpy Python toolbox for modeling neural responses to continuous stimuli.\n\n\n\n\n\nTuesday, the 19th of September, 2023\n\n\nOle Bialas\n\n\n\n\n\nNo matching items\n Back to top"
  },
  {
    "objectID": "posts/2024-02-23-eeg_preprocessing_2/index.html",
    "href": "posts/2024-02-23-eeg_preprocessing_2/index.html",
    "title": "EEG preprocessing II: eye-artifacts, repairing and rejecting",
    "section": "",
    "text": "The previous post on preprocessing EEG presented a minimally invasive pipeline of procedures that are necessary in most EEG analyses. In this post I present additional steps that might be useful if the data is still not sufficiently cleaned. First, I will address eye blinks which is one of the most prevalent sources of artifacts in EEG recordings. After that, I‚Äôll demonstrate a method to repair or remove segments of the data contaminated with noise."
  },
  {
    "objectID": "posts/2024-02-23-eeg_preprocessing_2/index.html#footnotes",
    "href": "posts/2024-02-23-eeg_preprocessing_2/index.html#footnotes",
    "title": "EEG preprocessing II: eye-artifacts, repairing and rejecting",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis is referred to as the corneo-retinal dipole. A explanation of the underlying physiology can be found in: Arden, G. B., & Constable, P. A. (2006). The electro-oculogram. Progress in retinal and eye research, 25(2), 207-248.‚Ü©Ô∏é\nA detailed investigation of eye-artifacts and their detection via ICA can be found in: Pl√∂chl, M., Ossand√≥n, J. P., & K√∂nig, P. (2012). Combining EEG and eye tracking: identification, characterization, and correction of eye movement artifacts in electroencephalographic data. Frontiers in human neuroscience, 6, 278.‚Ü©Ô∏é\nAn in-depth explanation of ICA is beyond the scope of this post but can be found in: Makeig, S., Bell, A., Jung, T. P., & Sejnowski, T. J. (1995). Independent component analysis of electroencephalographic data. Advances in neural information processing systems, 8.‚Ü©Ô∏é\nA highpass between 1 and 2 Hz before ICA is optimal, see Winkler, I., Debener, S., M√ºller, K. R., & Tangermann, M. (2015, August). On the influence of high-pass filtering on ICA-based artifact reduction in EEG-ERP. In 2015 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC) (pp.¬†4101-4105). IEEE.‚Ü©Ô∏é\nThe absolute sign of the component is meaningless and may change when ICA is performed repeatedly.‚Ü©Ô∏é\nA detailed description of the corrmap algorithm can be found in Viola, F. C., Thorne, J., Edmonds, B., Schneider, T., Eichele, T., & Debener, S. (2009). Semi-automatic identification of independent components representing EEG artifact. Clinical Neurophysiology, 120(5), 868-877.‚Ü©Ô∏é\nA detailed description of the autoreject algorithm can be found in Jas, M., Engemann, D. A., Bekhti, Y., Raimondo, F., & Gramfort, A. (2017). Autoreject: Automated artifact rejection for MEG and EEG data. NeuroImage, 159, 417-429.‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/2024-01-16-eeg_preprocessing_1/index.html",
    "href": "posts/2024-01-16-eeg_preprocessing_1/index.html",
    "title": "EEG preprocessing I: detrending, denoising and referencing",
    "section": "",
    "text": "Electroencephalography (EEG) measures brain activity via electrodes on the scalp. Unfortunately, those electrodes also picks up other things like muscle activity and electromagnetic interference that are orders of magnitude larger than neural responses. There is a vast literature and no consensus on how to deal with this problem."
  },
  {
    "objectID": "posts/2024-01-16-eeg_preprocessing_1/index.html#footnotes",
    "href": "posts/2024-01-16-eeg_preprocessing_1/index.html#footnotes",
    "title": "EEG preprocessing I: detrending, denoising and referencing",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nDelorme, A. (2023). EEG is better left alone. Scientific reports, 13(1), 2372.‚Ü©Ô∏é\nThe preprocessing steps described here apply to MEG as well - I just omitted it for the sake of simplicity.‚Ü©Ô∏é\nFor a detailed investigation of this issue see de Cheveign√©, A., & Nelken, I. (2019). Filters: when, why, and how (not) to use them. Neuron, 102(2), 280-293.‚Ü©Ô∏é\nThe detrending algorithm is described in: de Cheveign√©, A., & Arzounian, D. (2018). Robust detrending, rereferencing, outlier detection, and inpainting for multichannel data. NeuroImage, 172, 903-912.‚Ü©Ô∏é\nThe denoising algorithm is described in: de Cheveign√©, A. (2020). ZapLine: A simple and effective method to remove power line artifacts. NeuroImage, 207, 116356.‚Ü©Ô∏é\nRANSAC is part of another EEG preprocessing pipeline described in: Bigdely-Shamlo, N., Mullen, T., Kothe, C., Su, K. M., & Robbins, K. A. (2015). The PREP pipeline: standardized preprocessing for large-scale EEG analysis. Frontiers in neuroinformatics, 9, 16.‚Ü©Ô∏é"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Tracking Units in Chronic Electrophysiological Recordings with UnitMatch\n\n\nA demonstration of UnitMatch, a Bayesian algorithm that matches units based on average waveforms across recordings. I simulate multi-session electrophysiological data with SpikeInterface and track units across recordings.\n\n\n\n\n\nFriday, the 10th of October, 2025\n\n\nOle Bialas\n\n\n\n\n\n\n\n\n\n\n\n\nSurveying Neural Spiking in Visual Cortex with Pandas and Seaborn\n\n\nIn this post, I‚Äôll demonstrate how to use the Python libraries Pandas and Seaborn to ananlyze and plot spiking data from hundreds of neurons located in different areas of the visual cortex.\n\n\n\n\n\nWednesday, the 3rd of September, 2025\n\n\nOle Bialas\n\n\n\n\n\n\n\n\n\n\n\n\nEEG preprocessing II: eye-artifacts, repairing and rejecting\n\n\nThe second part of this series demonstrate additional preprocessing steps. Specifically, it addresses the problem of eye artifacts which are omnipresent in EEG recordings. It also demonstartes a procedure for repairing and rejecting noise-contaminated channels and segments.\n\n\n\n\n\nFriday, the 23rd of February, 2024\n\n\nOle Bialas\n\n\n\n\n\n\n\n\n\n\n\n\nEEG preprocessing I: detrending, denoising and referencing\n\n\nPreprocessing is an important and controversial topic in EEG research. Here, I discuss it‚Äôs necessity and present a minimal preprocessing pipeline that deals with the most common sources of noise while avoiding to distort the data. I demonstrate each step using publicly available data.\n\n\n\n\n\nTuesday, the 16th of January, 2024\n\n\nOle Bialas\n\n\n\n\n\nNo matching items\n Back to top"
  },
  {
    "objectID": "talks/2025-01-18-deutsches-museum/index.html",
    "href": "talks/2025-01-18-deutsches-museum/index.html",
    "title": "Zwischen Lab und Laptop - Technische Herausforderungen der Modernen Neurowissenschaft",
    "section": "",
    "text": "Slides: get the slides as PDF\n\n\n\n\n Back to top"
  },
  {
    "objectID": "talks/2024-04-08-bids/index.html",
    "href": "talks/2024-04-08-bids/index.html",
    "title": "Standardizing and Sharing EEG Data",
    "section": "",
    "text": "Slides: download the slides as PDF\n\n\n\n\n Back to top"
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications (6)",
    "section": "",
    "text": "Order By\n      Default\n      \n        Issued - Oldest\n      \n      \n        Issued - Newest\n      \n      \n        Title\n      \n      \n        Journal\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\nIssued\n\n\n\nTitle\n\n\n\nJournal\n\n\n\n\n\n\n\n\nMay,2025\n\n\nAppropriate data segmentation improves speech encoding models: Analysis and simulation of electrophysiological recordings\n\n\nPLoS One\n\n\n\n\n\n\nMay,2025\n\n\nElectrophysiological indices of acoustic and predictive language processing are reduced in schizophrenia spectrum disorder during socially relevant naturalistic video\n\n\nBiological Psychiatry\n\n\n\n\n\n\nAugust,2024\n\n\nCorrelates of syntax in EEG responses to naturalistic speech\n\n\nConference on Cognitive Computational Neuroscience\n\n\n\n\n\n\nAugust,2023\n\n\nmTRFpy: A python package for temporal response function analysis\n\n\nJournal of Open Source Software\n\n\n\n\n\n\nMay,2023\n\n\nEvoked responses to localized sounds suggest linear representation of elevation in human auditory cortex\n\n\nbioRxiv\n\n\n\n\n\n\nJune,2021\n\n\nS (ound) lab: An easy to learn python package for designing and running psychoacoustic experiments.\n\n\nJournal of Open Source Software\n\n\n\n\n\n\nNo matching items\n Back to top"
  },
  {
    "objectID": "talks/2025-08-20-datalad/index.html",
    "href": "talks/2025-08-20-datalad/index.html",
    "title": "Managing Scientific Data and Workflows with DataLad",
    "section": "",
    "text": "Slides and exercise notebooks: https://olebialas.github.io/DataLad-EuroScipy25/\nCode: https://github.com/OleBialas/DataLad-EuroScipy25\n\n\n\n\n Back to top"
  },
  {
    "objectID": "talks/2023-09-19-cnsp/index.html",
    "href": "talks/2023-09-19-cnsp/index.html",
    "title": "Fundamentals of TRF-analyses in Python",
    "section": "",
    "text": "Slides: download the slides as PDF\nmTRFpy documentation: https://mtrfpy.readthedocs.io/en/latest/\n\n\n\n\n Back to top"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Ole Bialas",
    "section": "",
    "text": "I am currently working as a research software consultant in the field of neuroscience at the University of Bonn in Germany where I am helping researchers to leverage technology for generating scientific insights. This includes things like data science, data management and software engineering practices (e.g.¬†version control, continuous integration).\nBefore that, I was an auditory neurscientistüß†, researching how the brain processes spatial sound and naturalistic speech. During this time, I also developed Python üêç packages to run psychoacoustic experiments (slab) and model brain responses to continuous speech (mTRFpy)\nIf you are interested in consulting ‚Äî send me an ‚úâ email (ole.bialas at posteo dot de), if you want to receive updates about the free online workshops that me and my colleagues are offering, subscribe to our üì¨ mailing list You may also want to take a look at my complete üìÑ curriculum vitae.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/2025-09-03-psth_with_pandas/index.html",
    "href": "posts/2025-09-03-psth_with_pandas/index.html",
    "title": "Surveying Neural Spiking in Visual Cortex with Pandas and Seaborn",
    "section": "",
    "text": "Modern electrophysiology equipment like the Neuropixels probes allows for simultaneous recording of hundreds of neurons. This leaves researchers with massive amounts of data to process and visualize. Fortunately, the Python ecosystem provides powerful tools for generating beautiful visualizations from large amounts of data in only a few lines of code. In this post, I‚Äôll use data from a 2021 study by Joshua Siegle and colleagues1, conducted at the Allen Institute, where they recorded tens of thousands of neurons from six different cortical regions across multiple experimental conditions.\nIn this post, I‚Äôll take the recordings from a single animal that was presented with bright flashes. I‚Äôll visualize the responses across cortical regions in peri-stimulus time histograms (PSTHs) using Pandas and Seaborn. Why Pandas and Seaborn? While these libraries are not geared towards electrophysiological data, they offer established powerful tools for analyzing large amounts of data. A good understanding of Pandas and Seaborn will also be useful in a variety of scenarios since these libraries are widely used in academia and industry.\nTo follow along with my examples, install the required packages and download the data as described in the next section."
  },
  {
    "objectID": "posts/2025-09-03-psth_with_pandas/index.html#prerequisites",
    "href": "posts/2025-09-03-psth_with_pandas/index.html#prerequisites",
    "title": "Surveying Neural Spiking in Visual Cortex with Pandas and Seaborn",
    "section": "Prerequisites",
    "text": "Prerequisites\nFirst we have to install the required modules using pip:\npip install numpy matplotlib seaborn pandas pyarrow\nThen, we can import the installed modules and download the data. We need two data frames ‚Äî one containing the timing of the presented stimuli and another one containing the recorded spikes. Conveniently, all read_ functions in Pandas accept URLs and can gracefully handle the downloading for us.\n\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n\ndf_stimuli = pd.read_parquet(\"https://uni-bonn.sciebo.de/s/G64EkHoQkeZeoLm/download\")\ndf_spikes = pd.read_parquet(\"https://uni-bonn.sciebo.de/s/mLMkb2TwbNx3Yg6/download\")"
  },
  {
    "objectID": "posts/2025-09-03-psth_with_pandas/index.html#referencing-spike-times-to-stimulus-presentations",
    "href": "posts/2025-09-03-psth_with_pandas/index.html#referencing-spike-times-to-stimulus-presentations",
    "title": "Surveying Neural Spiking in Visual Cortex with Pandas and Seaborn",
    "section": "Referencing Spike Times to Stimulus Presentations",
    "text": "Referencing Spike Times to Stimulus Presentations\nFirst, let‚Äôs take a look at the data. We have two data frames: df_stimuli contains the start and stop time for every stimulus 2 and df_spikes contains the unit ID, brain area and time for every recorded spike.\n\ndf_stimuli.head(5)\n\n\n\nTable¬†1: Stimuli\n\n\n\n\n\n\n\n\n\n\nstart_time\nstop_time\n\n\n\n\n0\n1276.297013\n1276.547221\n\n\n1\n1280.300363\n1280.550569\n\n\n2\n1286.305383\n1286.555591\n\n\n3\n1290.308743\n1290.558946\n\n\n4\n1294.312073\n1294.562279\n\n\n\n\n\n\n\n\n\n\n\ndf_spikes.head(5)\n\n\n\nTable¬†2: Recorded spikes\n\n\n\n\n\n\n\n\n\n\nunit_id\nbrain_area\nspike_time\n\n\n\n\n0\n951031334\nLM\n1276.297339\n\n\n1\n951031154\nLM\n1276.298206\n\n\n2\n951031243\nLM\n1276.298739\n\n\n3\n951021543\nPM\n1276.299007\n\n\n4\n951031253\nLM\n1276.301272\n\n\n\n\n\n\n\n\n\n\nTo be able to relate the spiking to the presented stimuli, we have to combine the information from Table¬†1 and Table¬†2. To do this, we first create a new column in the stimulus dataframe called \"analysis_window_start\"3 which is simply the stimulus onset minus 0.5 seconds. Then, we use pd.merge_asof() to match every spike with the closest analysis window. In the merged data frame df, we subtract the stimulus start time from the spike time which gives us the spike time relative to the stimulus. Finally, we remove all relative spike time values greater than 1 and remove the columns we don‚Äôt need. The final dataframe contains all spikes happening between 0.5 seconds before and 1.0 seconds after stimulus onset.\n\ndf_stimuli[\"analysis_window_start\"] = df_stimuli.start_time - 0.5\ndf = pd.merge_asof(\n    df_spikes, df_stimuli, left_on=\"spike_time\", right_on=\"analysis_window_start\"\n)\ndf.spike_time -= df.start_time\ndf = df[df.spike_time &lt;= 1.0]\ndf = df[[\"spike_time\", \"brain_area\", \"unit_id\"]]\ndf.sample(5)\n\n\n\nTable¬†3: Spike times relative to stimulus onset\n\n\n\n\n\n\n\n\n\n\nspike_time\nbrain_area\nunit_id\n\n\n\n\n365421\n0.000291\nPM\n951021314\n\n\n208777\n0.372630\nPM\n951021469\n\n\n108261\n0.081441\nAM\n951017432\n\n\n402572\n-0.250033\nAL\n951035799\n\n\n156489\n0.783751\nAL\n951035530"
  },
  {
    "objectID": "posts/2025-09-03-psth_with_pandas/index.html#creating-a-peri-stimulus-time-histogram",
    "href": "posts/2025-09-03-psth_with_pandas/index.html#creating-a-peri-stimulus-time-histogram",
    "title": "Surveying Neural Spiking in Visual Cortex with Pandas and Seaborn",
    "section": "Creating a Peri-Stimulus Time Histogram",
    "text": "Creating a Peri-Stimulus Time Histogram\nNow we can quantify how the spiking changes in response to the stimulus. To do this, we create an array of 10 millisecond wide bins between -0.5 and 1.0 seconds, group the data by unit ID and brain area and count the spike time values that fall into each bin. Finally, we reset the index of the returned series, take the middle of each bin and rename the columns. The resulting dataframe contains the spike count for every unit in every bin.\n\nbins = np.arange(-0.5, 1, 0.01)\npsth = (\n    df.groupby([\"unit_id\", \"brain_area\"])\n    .spike_time.value_counts(bins=bins)\n    .reset_index()\n)\npsth.spike_time = psth.spike_time.array.mid\npsth.columns = [\"unit_id\", \"brain_area\", \"bin_time\", \"spike_count\"]\npsth.sample(5)\n\n\n\nTable¬†4: Peri-stimulus time histogram\n\n\n\n\n\n\n\n\n\n\nunit_id\nbrain_area\nbin_time\nspike_count\n\n\n\n\n32133\n951027638\nV1\n0.415\n0\n\n\n44260\n951035018\nAL\n-0.455\n0\n\n\n17535\n951021725\nPM\n0.785\n2\n\n\n1466\n951015854\nAM\n0.265\n1\n\n\n4910\n951016603\nAM\n0.455\n0\n\n\n\n\n\n\n\n\n\n\nTo visualize the PSTH, we‚Äôll use seaborn‚Äôs relplot() function to plot the spike counts against the bin times. We can also add reference lines at 0 and 0.25 seconds that mark the stimulus onset and offset. There is a clear pattern ‚Äî spiking increases after the onset, falls back to baseline, increases again after the offset, drops below the baseline and then rebounds.\n\ng = sns.relplot(data=psth, x=\"bin_time\", y=\"spike_count\", kind=\"line\")\ng.refline(x=0, color=\"black\", linestyle=\"--\", alpha=0.5)\ng.refline(x=0.25, color=\"black\", linestyle=\"--\", alpha=0.5)\n\n\n\n\n\n\n\nFigure¬†1: Average time-binned spike count between 500 ms before and 1000 ms after stimulus onset. The shaded interval shows the 95% confidence-interval and the vertical dashed lines show the stimulus onset (0 s) and offset (0.25 s).\n\n\n\n\n\nWe can also add a hue to encode the brain area and zoom in on the x-axis in order to make out differences in the spiking profile between brain areas (we‚Äôll set errorbar=None because the overlapping confidence intervals will make the plot hard to read).\n\ng = sns.relplot(data=psth, x=\"bin_time\", y=\"spike_count\", kind=\"line\" , hue=\"brain_area\", errorbar=None)\ng.set(xlim=(-0.1, 0.5))\n\n\n\n\n\n\n\nFigure¬†2: Average time-binned spike count between 500 ms before and 1000 ms after stimulus onset for each visual cortex area.\n\n\n\n\n\nWe can see that spiking in the primary visual cortex V1 peaks first which is expected since it represents the lowest level in the visual processing hierarchy. However, there are large differences in the average firing rate between areas which makes this plot hard to read. To overcome this limitation we have to apply baseline correction."
  },
  {
    "objectID": "posts/2025-09-03-psth_with_pandas/index.html#applying-a-baseline-correction",
    "href": "posts/2025-09-03-psth_with_pandas/index.html#applying-a-baseline-correction",
    "title": "Surveying Neural Spiking in Visual Cortex with Pandas and Seaborn",
    "section": "Applying a Baseline Correction",
    "text": "Applying a Baseline Correction\nTo compute the baselines, we select all spikes that happen before 0 seconds (i.e.¬†the stimulus onset), group the data by unit ID and calculate the mean spike count to obtain an estimate of each unit‚Äôs average firing rate in absence of the stimulus. Then, we merge the baseline estimates with the PSTH and create a new column called \"spike_count_change\" by subtracting the baseline from the spike count.\n\nbaseline = psth[psth.bin_time &lt; 0].groupby([\"unit_id\"]).spike_count.mean()\nbaseline.name = \"baseline\"\npsth = psth.merge(baseline, on=\"unit_id\")\npsth[\"spike_count_change\"] = psth.spike_count - psth.baseline\npsth.sample(5)\n\n\n\nTable¬†5: Baseline-corrected spike counts\n\n\n\n\n\n\n\n\n\n\nunit_id\nbrain_area\nbin_time\nspike_count\nbaseline\nspike_count_change\n\n\n\n\n11666\n951020899\nPM\n-0.285\n9\n8.62\n0.38\n\n\n11462\n951018301\nAM\n0.885\n0\n0.00\n0.00\n\n\n44134\n951035003\nAL\n-0.345\n14\n13.62\n0.38\n\n\n644\n951015731\nAM\n-0.075\n0\n0.00\n0.00\n\n\n27814\n951026880\nV1\n-0.415\n11\n13.88\n-2.88\n\n\n\n\n\n\n\n\n\n\nNow we can recreate our area-specific PSTH plot using the spike count change ‚Äî the result looks much clearer!\n\ng = sns.relplot(\n    data=psth,\n    x=\"bin_time\",\n    y=\"spike_count_change\",\n    kind=\"line\",\n    hue=\"brain_area\",\n    errorbar=None,\n)\ng.set(xlim=(-0.1, 0.5))\n\n\n\n\n\n\n\nFigure¬†3: Baseline-corrected changes in spike count between 100 ms before and 500 ms after stimulus onset across brain areas.\n\n\n\n\n\nHowever, there is still a problem: the number of units that respond to the stimulus varies across brain areas as higher visual areas generally respond less to simple flashes which confounds our analysis. In order to interpret the absolute spike counts we need to identify the units that actually respond to the stimuli."
  },
  {
    "objectID": "posts/2025-09-03-psth_with_pandas/index.html#identifying-responsive-units",
    "href": "posts/2025-09-03-psth_with_pandas/index.html#identifying-responsive-units",
    "title": "Surveying Neural Spiking in Visual Cortex with Pandas and Seaborn",
    "section": "Identifying Responsive Units",
    "text": "Identifying Responsive Units\nTo identify the responsive units, we first compute the relative change in spiking by dividing the baseline subtracted spikecount by the baseline. We add a small regularization coefficient \\(\\epsilon\\) to avoid that baseline values close to 0 inflate the estimate 4. Then, we can plot the relative change in spiking for every unit (this requires setting estimator=None). Let‚Äôs add a horizontal line to mark the threshold for responsiveness ‚Äî in this example, we want to consider a unit responsive if its relative spike count increases 5 times with respect to the baseline. This method of identifying responsive units is not statistically rigorous, but it is a quick and convenient way of filtering the data for visualization.\n\nepsilon = 0.5\nthreshold = 5\npsth[\"rel_spike_count_change\"] = (psth.spike_count - psth.baseline) / (\n    psth.baseline + epsilon\n)\ng = sns.relplot(\n    psth,\n    x=\"bin_time\",\n    y=\"rel_spike_count_change\",\n    units=\"unit_id\",\n    kind=\"line\",\n    estimator=None,\n)\ng.refline(y=threshold, color=\"black\", linestyle=\"--\", alpha=0.5)\n\n\n\n\n\n\n\nFigure¬†4: Each unit‚Äôs change in the time-binned spike counts relative to the baseline. The horizontal line marks the threshold above which a unit is considered responsive.\n\n\n\n\n\nNow, we group the data by unit ID and check whether each unit‚Äôs maximum value for the relative change in spike count is above the selected threshold. Then, we merge the result with the PSTH to get a new column with boolean values that indicate whether any given unit is responsive. Finally, we can reproduce Figure¬†3 selecting only the units that are responsive according to our threshold criterion.\n\nis_responsive = psth.groupby([\"unit_id\"]).rel_spike_count_change.max() &gt; threshold\nis_responsive.name = \"is_responsive\"\npsth = psth.merge(is_responsive, on=\"unit_id\")\ng = sns.relplot(\n    data=psth[psth.is_responsive],\n    x=\"bin_time\",\n    y=\"spike_count_change\",\n    kind=\"line\",\n    hue=\"brain_area\",\n    errorbar=None,\n)\ng.set(xlim=(-0.1, 0.5))\n\n\n\n\n\n\n\nFigure¬†5: Baseline-corrected changes in spike count in units with an above-threshold response to the stimuli. Note that there are only 5 brain areas since no neuron from area RL exceeded the threshold.\n\n\n\n\n\nNow the differences in response latency between the areas are much clearer. We can also make out some interesting trends ‚Äî for example, units in V1 appear to respond mostly to the stimulus onset while units in LM respond more strongly to the offset."
  },
  {
    "objectID": "posts/2025-09-03-psth_with_pandas/index.html#closing-remarks",
    "href": "posts/2025-09-03-psth_with_pandas/index.html#closing-remarks",
    "title": "Surveying Neural Spiking in Visual Cortex with Pandas and Seaborn",
    "section": "Closing Remarks",
    "text": "Closing Remarks\nThe goal of this post was to show how you can use Pandas and Seaborn to create detailed and informative graphs from large datasets (i.e.¬†tables with hundreds of thousands of rows) with relatively little code. The methods for data aggregation, merging and visualization shown here will be applicable to any data that can be arranged in a tabular format. What this demo lacks is a statistical comparison of the spiking activity between brain areas. This was outside of the scope of this post since I only used data from a single animal. If you are interested in working with the full dataset you can check out my GitHub repo which contains the recordings from over 50 animals in the same data format used in this demo."
  },
  {
    "objectID": "posts/2025-09-03-psth_with_pandas/index.html#footnotes",
    "href": "posts/2025-09-03-psth_with_pandas/index.html#footnotes",
    "title": "Surveying Neural Spiking in Visual Cortex with Pandas and Seaborn",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSiegle, J. H., Jia, X., Durand, S., Gale, S., Bennett, C., Graddis, N., ‚Ä¶ & Koch, C. (2021). Survey of spiking in the mouse visual system reveals functional hierarchy. Nature, 592(7852), 86-92.‚Ü©Ô∏é\nAll stimuli are full-field flashes so there isn‚Äôt any relevant information besides the stimuli‚Äôs timing‚Ü©Ô∏é\nWe could skip creating the \"analysis_window_start\" column and merge the data using the stimulus onsets. However, then we would lose the spikes that occur right before the onset which we need to compute the baseline.‚Ü©Ô∏é\nThe exact value of \\(\\epsilon\\) is not that relevant here, so feel free to try out different values. However, be aware that larger values of \\(\\epsilon\\) will lead to smaller values of the relative change in spike count for all units, so you‚Äôll have to adapt the threshold as well.‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/2024-10-02-unitmatch/index.html",
    "href": "posts/2024-10-02-unitmatch/index.html",
    "title": "Tracking Units in Chronic Electrophysiological Recordings with UnitMatch",
    "section": "",
    "text": "I recently consulted on a project where chronic recordings from mice were made with implanted electrodes. This presents challenges: processing multi-day recordings in one go is highly inefficient or impossible due to memory limitations. While dividing data into chunks enables parallel processing, it creates a new problem: the units identified in different recording segments do not follow the same order (e.g.¬†unit 1 in recording 1 is likely not the same as unit 1 in recording 2). Thus, the units must somehow be matched across recordings based on physiological features.\nWhile searching for solutions to this problem, I came across UnitMatch - an algorithm that uses a naive Bayes classifier to match units based on their average spike waveforms (more on that later). The algorithm was published in Nature Methods and there is a Matlab and a Python toolbox available. Unfortunately, I found that the project lacks documentation and existing tutorial notebooks contained errors or were incomplete (due to missing data).\nTo test and verify the functionalities of UnitMatch, I created this notebook where I simulate multiple electrophysiological recordings with SpikeInterface and identify matching units across recordings. I hope prospective UnitMatch users will find it useful. Feel free to play around with the parameters of the simulation and UnitMatch to see how it affects the results. To follow along with the examples, you‚Äôll have to install the following packages (I recommend using a new virtual environment).\nfrom pathlib import Path\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport spikeinterface.core as si\nimport spikeinterface.preprocessing as spre\nfrom spikeinterface.core.generate import (\n    generate_unit_locations,\n    generate_templates,\n    generate_ground_truth_recording,\n    default_unit_params_range,\n)\nfrom spikeinterface.curation import MergeUnitsSorting\nfrom spikeinterface import aggregate_units\nfrom probeinterface import generate_multi_columns_probe\nfrom probeinterface.plotting import plot_probe\nimport UnitMatchPy as um\nimport UnitMatchPy.default_params as default_params"
  },
  {
    "objectID": "posts/2024-10-02-unitmatch/index.html#preprocessing-and-curation",
    "href": "posts/2024-10-02-unitmatch/index.html#preprocessing-and-curation",
    "title": "Tracking Units in Chronic Electrophysiological Recordings with UnitMatch",
    "section": "Preprocessing and Curation",
    "text": "Preprocessing and Curation\nSince UnitMatch uses average unit waveforms, we must ensure they‚Äôre clearly visible across multiple recording sites. To achieve this, we‚Äôll preprocess the recordings and select units based on quality criteria. For the simulated data, I am only applying a bandpass filter, but real data may require additional steps such as correcting the small sampling delay across channels that some systems (e.g.¬†Neuropixels) introduce or applying motion correction (note that this only corrects motion within, not across, recordings).\n\nfor i, recording in enumerate(recordings):\n    recording = spre.bandpass_filter(recording, freq_min=300, freq_max=3000)\n    # recording = spre.phase_shift(recording)\n    # recording = spre.correct_motion(recording, preset=\"nonrigid_fast_and_accurate\")\n    recordings[i] = recording\nrecordings\n\n[GroundTruthRecording (BandpassFilterRecording): 12 channels - 25.0kHz - 1 segments \n                       250,000 samples - 10.00s - float32 dtype - 11.44 MiB,\n GroundTruthRecording (BandpassFilterRecording): 12 channels - 25.0kHz - 1 segments \n                       250,000 samples - 10.00s - float32 dtype - 11.44 MiB,\n GroundTruthRecording (BandpassFilterRecording): 12 channels - 25.0kHz - 1 segments \n                       250,000 samples - 10.00s - float32 dtype - 11.44 MiB]\n\n\nNext, we calculate waveform templates from the recordings. To do this, we create an analyzer object by pairing each recording and sorting and compute ‚Äúextensions‚Äù that are added to the analyzer. These extensions partly depend on each other ‚Äî for example, computing the \"templates\" extension requires the \"random_spikes\" extension to be computed. Important: UnitMatch expects the waveform to be symmetric around the spike, so when you use different values for ms_before and ms_after, you‚Äôll have to set the respective parameter in UnitMatch.\n\nanalyzers = []\nfor sorting, recording in zip(sortings, recordings):\n    analyzer = si.create_sorting_analyzer(sorting, recording, sparse=False)\n    analyzer.compute(\"random_spikes\", method=\"uniform\", max_spikes_per_unit=500)\n    analyzer.compute(\"templates\", ms_before=2, ms_after=2)\n    analyzers.append(analyzer)\nanalyzers\n\n\n\n\n\n\n\n\n\n\n[SortingAnalyzer: 12 channels - 10 units - 1 segments - memory - has recording\n Loaded 2 extensions: random_spikes, templates,\n SortingAnalyzer: 12 channels - 10 units - 1 segments - memory - has recording\n Loaded 2 extensions: random_spikes, templates,\n SortingAnalyzer: 12 channels - 10 units - 1 segments - memory - has recording\n Loaded 2 extensions: random_spikes, templates]\n\n\nLet‚Äôs look at two of the extracted templates. In the figure below, we can clearly see one unit that has a large amplitude and is visible across all channels, and another one with a weak and noisy signal.\n\ntemplates = analyzers[0].get_extension(\"templates\").get_data()\nfig, ax = plt.subplots(1, 2, figsize=(6, 3), sharex=True, sharey=False)\nfor i, uid in enumerate([0, 3]):\n    ax[i].plot(templates[uid, :, :])\n    ax[i].set(title=f\"Unit {uid+1}\")\nax[0].set(ylabel=r\"Voltage [$\\mu$V]\")\nax[1].set(xlabel=\"Time [samples]\")\n\n\n\n\n\n\n\nFigure¬†3: The average waveforms of two units extracted from recording 1.\n\n\n\n\n\nTo remove the latter unit and others like it, we compute \"quality_metrics\" and select only those units that meet our quality criteria. Since most quality criteria (like ISI violations) don‚Äôt apply to simulated data, we‚Äôll use only signal-to-noise ratio for unit selection. However, for real data that is probably insufficient ‚Äî I recommend this notebook for an overview and discussion of different quality criteria.\n\nmin_snr = 10\nfor i, analyzer in enumerate(analyzers):\n    analyzer.compute(\"noise_levels\")  # required to compute SNR\n    analyzer.compute(\"quality_metrics\", metric_names=[\"snr\"])\n    metrics = analyzer.get_extension(\"quality_metrics\").get_data()\n    keep_mask = metrics.snr &gt; min_snr\n    keep_unit_ids = sorting.unit_ids[keep_mask]\n    print(f\"Keeping {len(keep_unit_ids)} out of {len(sorting.unit_ids)} units\")\n    sortings[i] = sortings[i].select_units(keep_unit_ids)\n\nfig, ax = plt.subplots()\nax.hist(metrics.snr)\nax.axvline(x=min_snr, ymin=0, ymax=1, color=\"red\", linestyle=\"--\")\nax.set(ylabel=\"Number of Units\", xlabel=\"SNR\", title=\"SNR Distribution in Recording 3\")\n\nKeeping 8 out of 10 units\n\n\nKeeping 8 out of 10 units\n\n\nKeeping 8 out of 10 units\n\n\n\n\n\n\n\n\n\n\n(a) Histogram of the signal-to-noise ratio across all units. Units with a SNR below the red dashed line are removed.\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\n\n\n(c)\n\n\n\n\n\n\n[Text(0, 0.5, 'Number of Units'),\n Text(0.5, 0, 'SNR'),\n Text(0.5, 1.0, 'SNR Distribution in Recording 3')]\n\n\n(d)\n\n\n\n\n\n\n\n\n\n\n(e)\n\n\n\n\n\n\nFigure¬†4"
  },
  {
    "objectID": "posts/2024-10-02-unitmatch/index.html#preparing-the-data-for-unitmatch",
    "href": "posts/2024-10-02-unitmatch/index.html#preparing-the-data-for-unitmatch",
    "title": "Tracking Units in Chronic Electrophysiological Recordings with UnitMatch",
    "section": "Preparing the Data for UnitMatch",
    "text": "Preparing the Data for UnitMatch\nNext, we convert SpikeInterface data to UnitMatch‚Äôs expected format. First, we create a directory \"UMInputData\" with a subdirectory for each session and a \"RawWaveforms\" subdirectory in each session‚Äôs directory.\n\nUM_input_dir = Path(\"UMInputData\").absolute()\nfor i in range(len(recordings)):\n    session_dir = UM_input_dir / f\"Session{i+1}\"\n    (session_dir / \"RawWaveforms\").mkdir(parents=True, exist_ok=True)\n!tree UMInputData\n\n\nUMInputData\n\n‚îú‚îÄ‚îÄ Session1\n\n‚îÇ¬†¬† ‚îî‚îÄ‚îÄ RawWaveforms\n\n‚îú‚îÄ‚îÄ Session2\n\n‚îÇ¬†¬† ‚îî‚îÄ‚îÄ RawWaveforms\n\n‚îî‚îÄ‚îÄ Session3\n\n    ‚îî‚îÄ‚îÄ RawWaveforms\n\n\n\n7 directories, 0 files\n\n\n\n\nNext, we need to extract the channel locations from every recording and store them in the session directory as \"channel_positions.npy\".\n\nfor i, (recording, sorting) in enumerate(zip(recordings, sortings)):\n    channel_locations = recording.get_channel_locations()\n    np.save(UM_input_dir / f\"Session{i+1}\" / \"channel_positions.npy\", channel_locations)\n\nNext, we create a table for each recording storing the original cluster ID of each unit (as assigned by the spike sorter) and the ‚Äúgroup‚Äù to which it belongs. For the simulated data, all units are labeled \"good\" but for actual recordings you may want to label some as \"mua\" (multi-unit activity). The table is stored in the session directory as \"cluster_group.tsv\".\n\nfor i, (recording, sorting) in enumerate(zip(recordings, sortings)):\n    n_units = sorting.get_num_units()\n    cluster_group = np.stack((range(n_units), [\"good\" for i in range(n_units)]), axis=1)\n    cluster_group = np.vstack((np.array((\"cluster_id\", \"group\")), cluster_group))\n    np.savetxt(\n        UM_input_dir / f\"Session{i+1}\" / \"cluster_group.tsv\", cluster_group, fmt=\"%s\", delimiter=\"\\t\"\n    )\ncluster_group\n\narray([['cluster_id', 'group'],\n       ['0', 'good'],\n       ['1', 'good'],\n       ['2', 'good'],\n       ['3', 'good'],\n       ['4', 'good'],\n       ['5', 'good'],\n       ['6', 'good'],\n       ['7', 'good']], dtype='&lt;U21')\n\n\nNow comes the critical part: each recording and sorting has to be divided into two parts. UnitMatch calibrates the matching threshold by comparing units between the first and second half of each recording. We‚Äôll split every recording and sorting into two and append the pieces to lists. From this, we obtain the lists of lists split_recordings and split_sortings where each element is a list with the first and second half of a given sorting/recording.\n\nsplit_recordings, split_sortings = [], []\nfor sorting, recording in zip(sortings, recordings):\n    n = recording.get_num_samples()\n    split_sortings.append(  # split sortings\n        [\n            sorting.frame_slice(start_frame=0, end_frame=n // 2),  # 1st half\n            sorting.frame_slice(start_frame=n // 2, end_frame=n),  # 2nd half\n        ]\n    )\n    split_recordings.append(  # split recordings\n        [\n            recording.frame_slice(start_frame=0, end_frame=n // 2),  # 1st half\n            recording.frame_slice(start_frame=n // 2, end_frame=n),  # 2nd half\n        ]\n    )\nsplit_sortings\n\n[[GroundTruthSorting (FrameSliceSorting): 8 units - 1 segments - 25.0kHz,\n  GroundTruthSorting (FrameSliceSorting): 8 units - 1 segments - 25.0kHz],\n [GroundTruthSorting (FrameSliceSorting): 8 units - 1 segments - 25.0kHz,\n  GroundTruthSorting (FrameSliceSorting): 8 units - 1 segments - 25.0kHz],\n [GroundTruthSorting (FrameSliceSorting): 8 units - 1 segments - 25.0kHz,\n  GroundTruthSorting (FrameSliceSorting): 8 units - 1 segments - 25.0kHz]]\n\n\nFor the half recordings and sortings, we‚Äôll have to create new analyzers. The analyzers are stored in another list of lists called split_analyzers.\n\nsplit_analyzers = []\nfor recording, sorting in zip(split_recordings, split_sortings):\n    split_analyzers.append(\n        [\n            si.create_sorting_analyzer(sorting[0], recording[0], sparse=False),\n            si.create_sorting_analyzer(sorting[1], recording[1], sparse=False),\n        ]\n    )\nsplit_analyzers\n\n[[SortingAnalyzer: 12 channels - 8 units - 1 segments - memory - has recording\n  Loaded 0 extensions,\n  SortingAnalyzer: 12 channels - 8 units - 1 segments - memory - has recording\n  Loaded 0 extensions],\n [SortingAnalyzer: 12 channels - 8 units - 1 segments - memory - has recording\n  Loaded 0 extensions,\n  SortingAnalyzer: 12 channels - 8 units - 1 segments - memory - has recording\n  Loaded 0 extensions],\n [SortingAnalyzer: 12 channels - 8 units - 1 segments - memory - has recording\n  Loaded 0 extensions,\n  SortingAnalyzer: 12 channels - 8 units - 1 segments - memory - has recording\n  Loaded 0 extensions]]\n\n\nWe extract template waveforms from each half recording using the split_analyzers. We‚Äôll compute the \"templates\" extension on every analyzer, stack the waveforms of the first and second half into a single array and append it to a list called all_waveforms. Each element in all_waveforms is a 4-dimensional numpy array where the dimensions represent units, samples, channels, and halves.\n\nall_waveforms = []\nfor analyzer in split_analyzers:\n    # waveforms for 1st half\n    analyzer[0].compute(\"random_spikes\", method=\"uniform\", max_spikes_per_unit=500)\n    analyzer[0].compute(\"templates\", ms_before=2, ms_after=2, n_jobs=0.8)\n    # waveforms for 2nd half\n    analyzer[1].compute(\"random_spikes\", method=\"uniform\", max_spikes_per_unit=500)\n    analyzer[1].compute(\"templates\", ms_before=2, ms_after=2, n_jobs=0.8)\n    # get waveform arrays and append them to list\n    all_waveforms.append(\n        np.stack(\n            (\n                analyzer[0].get_extension(\"templates\").get_data(),\n                analyzer[1].get_extension(\"templates\").get_data(),\n            ),\n            axis=-1,\n        )\n    )\nall_waveforms[0].shape\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(8, 100, 12, 2)\n\n\nFinally, we can save the waveforms to the session folders. For this, we‚Äôll use the save_avg_waveforms function from UnitMatchPy. This function takes as arguments the average waveforms for a given session, the path to the session directory, as well as the previously created cluster_group table (so we‚Äôll have to load that again). The save_avg_waveforms function will store one numpy array for every unit in the RawWaveforms subdirectory within the session directory.\n\nfor i in range(len(all_waveforms)):\n    session_dir = UM_input_dir / f\"Session{i+1}\"\n    cluster_group = np.loadtxt(  # load table with good units\n        session_dir / \"cluster_group.tsv\", delimiter=\"\\t\", dtype=str\n    )\n    um.extract_raw_data.save_avg_waveforms(\n        all_waveforms[i],\n        session_dir,\n        cluster_group,\n    )\n!tree UMInputData/Session1\n\n\nSaved 9 units to RawWaveforms directory, saving all units\n\nSaved 9 units to RawWaveforms directory, saving all units\n\nSaved 9 units to RawWaveforms directory, saving all units\n\nUMInputData/Session1\n\n‚îú‚îÄ‚îÄ RawWaveforms\n\n‚îÇ¬†¬† ‚îú‚îÄ‚îÄ Unit0_RawSpikes.npy\n\n‚îÇ¬†¬† ‚îú‚îÄ‚îÄ Unit1_RawSpikes.npy\n\n‚îÇ¬†¬† ‚îú‚îÄ‚îÄ Unit2_RawSpikes.npy\n\n‚îÇ¬†¬† ‚îú‚îÄ‚îÄ Unit3_RawSpikes.npy\n\n‚îÇ¬†¬† ‚îú‚îÄ‚îÄ Unit4_RawSpikes.npy\n\n‚îÇ¬†¬† ‚îú‚îÄ‚îÄ Unit5_RawSpikes.npy\n\n‚îÇ¬†¬† ‚îú‚îÄ‚îÄ Unit6_RawSpikes.npy\n\n‚îÇ¬†¬† ‚îî‚îÄ‚îÄ Unit7_RawSpikes.npy\n\n‚îú‚îÄ‚îÄ channel_positions.npy\n\n‚îî‚îÄ‚îÄ cluster_group.tsv\n\n\n\n2 directories, 10 files"
  },
  {
    "objectID": "posts/2024-10-02-unitmatch/index.html#combining-matches-across-recordings",
    "href": "posts/2024-10-02-unitmatch/index.html#combining-matches-across-recordings",
    "title": "Tracking Units in Chronic Electrophysiological Recordings with UnitMatch",
    "section": "Combining Matches Across Recordings",
    "text": "Combining Matches Across Recordings\nWhile the probability matrix shows pairwise matching probabilities, our goal is tracking units across recordings. This is what the assign_unique_id function does ‚Äî it groups units identified as matches across recordings while applying three different strategies: - Liberal: Groups units if they match with ANY unit in the proposed group. - Default: Groups units if they match with EVERY unit in adjacent sessions. - Conservative: Groups units only if they match with EVERY other unit in the proposed group.\nIn our case, the default and conservative strategies are almost identical since there are only 3 sessions, but for a larger number of sessions there should be a notable difference. We‚Äôll proceed with the liberal strategy, although the original paper recommends using the conservative strategy.\n\nmatches = np.argwhere(output_threshold == 1)\nUIDs = um.assign_unique_id.assign_unique_id(output_prob_matrix, param, clus_info)\nunique_id = UIDs[0]  # liberal strategy\nunique_id\n\nNumber of Liberal Matches: 26\nNumber of Intermediate Matches: 21\nNumber of Conservative Matches: 21\n\n\narray([ 0,  1,  2,  3,  4,  5,  5,  7,  0,  1,  2,  3, 12,  5,  5,  7,  0,\n        1,  2,  3, 20,  5,  5,  7])\n\n\nNow, we combine all sortings using the aggregate_units function. Then, we identify matching units by identifying the locations of duplicates in the unique_id array. Finally, we use MergeUnitsSorting to merge and rename the units that have been matched. That‚Äôs it ‚Äî now we have a sorting object that combines units across all sessions. The combined recording has 10 units despite only 8 surviving initial quality control. This shows that we were not able to find all existing matches (you can also see this from the missing squares in Figure¬†9).\n\nsorting = aggregate_units(sortings)\nunits_to_merge = []\nnew_unit_ids = []\nfor uid in np.unique(unique_id):\n    idx = np.where(unique_id == uid)[0]\n    if len(idx) &gt; 1:\n        units_to_merge.append(list(sorting.unit_ids[idx]))\n        new_unit_ids.append(uid)\n\nsorting = MergeUnitsSorting(\n    sorting, units_to_merge=units_to_merge, new_unit_ids=new_unit_ids\n)\nprint(f\"After merging there are {sorting.get_num_units()} units: \\n{sorting.unit_ids}\")\n\nAfter merging there are 9 units: \n['4' '12' '20' '0' '1' '2' '3' '5' '7']\n\n\nUnitMatch also provides a save_to_output function that stores all of the estimated properties, distributions and probabilities. The cell below unpacks the extracted_wave_properties dictionary and saves the results to a folder called \"out\".\n\nsave_dir = \"out\"\namplitude = extracted_wave_properties[\"amplitude\"]\nspatial_decay = extracted_wave_properties[\"spatial_decay\"]\navg_centroid = extracted_wave_properties[\"avg_centroid\"]\navg_waveform = extracted_wave_properties[\"avg_waveform\"]\navg_waveform_per_tp = extracted_wave_properties[\"avg_waveform_per_tp\"]\nwave_idx = extracted_wave_properties[\"good_wave_idxs\"]\nmax_site = extracted_wave_properties[\"max_site\"]\nmax_site_mean = extracted_wave_properties[\"max_site_mean\"]\num.save_utils.save_to_output(\n    save_dir,\n    scores_to_include,\n    matches,\n    output_prob_matrix,\n    avg_centroid,\n    avg_waveform,\n    avg_waveform_per_tp,\n    max_site,\n    total_score,\n    output_threshold,\n    clus_info,\n    param,\n    UIDs=UIDs,\n    save_match_table=True,\n)"
  },
  {
    "objectID": "drafts/matplotlib/index.html",
    "href": "drafts/matplotlib/index.html",
    "title": "Prerequisites",
    "section": "",
    "text": "+++ title = ‚ÄòMaking publication-ready figures with Matplotlib‚Äô date = 2023-10-01T21:37:12-04:00 draft = false tags = [‚ÄòPython‚Äô, ‚Äòscience‚Äô, ‚Äòvisualization‚Äô] cover = ‚Äòposts/matplotlib/plot5.png‚Äô +++\n‚ÄúAn image says more than a thousand words‚Äù is a platitude, but when it comes to communicating the results of your research it is definitely true. Figures are probably the most important part of a paper and most readers will first look at them before reading the text in detail. In this blog post I‚Äôll show how to use the Python library Matplotlib for creating publication-ready figures. For this purpose I‚Äôll reproduce a figure from a recent paper step-by-step.\n\nPrerequisites\nIn this blog post I will use the Python library Matplotlib and reproduce the figure in the title step-by-step. If you want to follow along, you can download the data by clicking here or use Python to fetch it:\npip install matplotlib numpy mne\nThe package mne is only required to draw the scalp map (a common visualization in EEG research) in panel B of the figure. If you don‚Äôt want to reproduce that subplot, you may skip installing mne. You‚Äôll also need the data which you can download by clicking here or fetch using Python:\nimport requests\nfrom io import BytesIO\nimport numpy as np\n\nresponse = requests.get(\"https://olebialas.github.io/example_data.npy\")\nd = np.load(BytesIO(response.content), allow_pickle=True).item()\n\n\nData\nThe data contains the neural activity, and some derived statistics, of participants who localized sounds played from different locations. Because this blog post focuses on visualization, I‚Äôll not explain the scientific details. If you are interested you may have a look at the paper. Let‚Äôs look at the content of our data by printing the dictionary‚Äôs keys and the shape of the array stored under them:\nfor key, value in d.items():\n print(f\"{key}: {value.shape}\")\ntimes: (269,)\neeg_avg: (269, 64)\neeg_con: (269, 4)\ncon: (4,)\nch_f: (64,)\nch_loc: (64, 2)\nn_sig: (269,)\nline_y: (10000, 100)\nline_x: (100,)\n\ntimes is a vector of time points at which the EEG data are sampled\neeg_avg is a matrix with the average EEG recording of each channel\neeg_con contains the recording of a single channel for each condition.\ncon stores the conditions (i.e.¬†the elevation of the different sound sources)\nch_f and ch_loc contain the F-values (a statistic indicating the separation of responses to the different conditions) and x,y coordinates for each EEG channel.\nn_sig contains the number of subjects for which a statistical test found a significant difference between conditions at each point in time.\nline_y contains samples from a linear model for the relationship between sound source elevation and neural response amplitude\nline_x contains the x-coordinates (i.e.¬†elevations) the values where sampled at\n\n\n\nStyle templates\nMatplotlib is great, but the default style is not exactly pleasing to the eye. Fortunately there are some fantastic templates that you can use instead. I like to use ‚ÄúSciencePlots‚Äù which has an elegant and professional look and even offers the option to emulate the style of several academic journals. To use the style template, install it as a package with pip install SciencePlots, import it and set it as Matplotlib‚Äôs style:\nfrom matplotlib import pyplot as plt\nimport scienceplots\n\nplt.style.use('science')\n\n\nSubplot spacing\nI want to allocate most of the figure‚Äôs space to the time series data in panel A, while the scalp map in B and the regression line in C can smaller. We can create subplots of different size using the subplot_mosaic() function. This function takes a nested lists of strings where each unique string represents one subplots and it‚Äôs repetitions define the fraction of the figure that subplot occupies. Its a good idea to use capital letters because we can use those later to label the panels.\nfig, ax = plt.subplot_mosaic(\n    [\n        [\"A\", \"A\", \"A\", \"B\"],\n        [\"A\", \"A\", \"A\", \"C\"],\n    ],\n    figsize = (8, 4.5)\n)\nplt.subplots_adjust(wspace=0.25, hspace=0.15)\nThis creates a figure where panel A spans two rows and three columns while B and C only occupy a single element. The function subplots_adjust changes the spacing by adjusting the width (wspace) and height (hspace) of subplots‚Äô padding.\n\n\n\nLayout\n\n\n\n\nHighlighting what‚Äôs important\nLet‚Äôs plot the time series data to panel A. First I plot the average EEG for each recorded channel in gray with small line width. Next, I iterate through the four experimental conditions and plot each one individually which makes Matplotlib use a different color for each. Using gray and colors as well as different line width is a good way of giving a detailed overview of the data while highlighting important aspects.\nax[\"A\"].plot(d[\"times\"], d[\"eeg_avg\"], color=\"gray\", linewidth=0.3)\nfor i_con in range(d[\"eeg_con\"].shape[1]):\n    ax[\"A\"].plot(\n        d[\"times\"], d[\"eeg_con\"][:, i_con],\n        linewidth=2, label=f\"{d['con'][i_con]} [$^\\circ$]\"\n        )\nax[\"A\"].legend(loc=(0.25, 0.07))\nax[\"A\"].set(ylabel=\"Amplitude [$\\mu$ V]\", xlim=(d[\"times\"].min(), d[\"times\"].max()))\nBy labeling each conditions line with the respective sources elevation, we can create an informative legend. The last line is labeling the y-axis and adjusting the x-axis to the recorded time interval. Putting dollar signs around string, tells Matplotlib to render them with LateX which allows us to use equations and special symbols like Greek letters. If you don‚Äôt have a LateX installed, you‚Äôll have to change those strings.\n\n\nMarking time points\nIn the experiment, participants heard two subsequent sounds. Let‚Äôs mark the sound onsets so we can see how the neural response relates to the stimuli. The .axvline() method takes a point on the x-axis and draws a vertical line between ymin and ymax. Here, x is in data coordinates (i.e.¬†seconds) while the y coordinates are expressed as a fraction of the axis.\nax[\"A\"].axvline(x=0, ymin=0, ymax=1, color=\"black\", linestyle=\"--\")\nax[\"A\"].axvline(x=-1, ymin=0, ymax=1, color=\"gray\", linestyle=\"--\")\n\n\n\nPlot 1\n\n\n\n\nAdding subplots\nIn the study, we ran a statistical test for differences between the responses to the different sound sources at each point in time. I want to visualize the number of participants for whom the test found a significant result using a heat map. To do this, we can use the makes_axes_locatable() function which returns an AxesDivider that allows us to split off a new subplot\nfrom mpl_toolkits.axes_grid1 import make_axes_locatable\n\ndivider = make_axes_locatable(ax[\"A\"])\nnew_ax = divider.append_axes(\"bottom\", size=\"6%\", pad=0)\nThis adds a subplot to the bottom of panel A whose size is equal to six percent of the original subplot and adds it to the ax dictionary. Now we can plot our heat map to the new subplot.\nextent = [d[\"times\"].min(), d[\"times\"].max(), 0, 1]\nim = new_ax.imshow(d[\"n_sig\"], aspect=\"auto\", extent=extent)\nnew_ax.set(xlabel=\"Time [s]\", yticks=[])\nThe images extent are the data coordinates that the image will fill and setting the aspect=\"auto\" allows non-square pixels to fit the axis.\nNow the heat map needs a color bar that explains what the colors represent. The color bar should be short and tucked to the side of the heat map. The most easy way to create a subplot is to directly define its location. The add_axis() method takes as inputs the x and y coordinate of the new subplot‚Äôs left bottom corner and its width and height relative to the whole figure.\nAfter creating the subplot we can use the colorbar() function on the image returned by imshow(). Finally, we can adjust the font and tick size.\ncax = fig.add_axes([0.7, 0.11, 0.01, 0.048])\nfig.colorbar(im, cax=cax, orientation=\"vertical\", ticks=[0, d[\"n_sig\"].max()])\ncax.set_title(\"$N$\", fontsize=\"medium\")\ncax.tick_params(labelsize=8)\n\n\n\nPlot 2\n\n\nThe downside of manually positioning the color bar is that you have to try around to find the right coordinates. This can be made a little easier using Matplotlib‚Äôs interactive mode by calling plt.ion(). In interactive mode, all plotting commands are immediately executed so you can see where an object is located.\n\n\nHiding the axis\nThe scalp plot in panel B doesn‚Äôt need the surrounding axes. We can turn them off before plotting. Then we can create the scalp map, using a function from the MNE toolbox and add a color bar just as we did before.\nfrom mne.viz import plot_topomap\nim, _ = plot_topomap(d[\"ch_f\"], d[\"ch_loc\"], show=False, axes=ax[\"B\"])\ncax = fig.add_axes([0.91, 0.65, 0.01, 0.2])\ncbar = fig.colorbar(im, cax=cax, orientation=\"vertical\")\ncbar.set_label(\"$F-score$\")\n\n\n\nPlot 3\n\n\n\n\nDisplaying uncertainty\nPanel C contains samples from a linear model which describes the relationship between neural response amplitude and sound elevation. The regression model was computed 10k times using bootstrapping and the variability across these models indicates the uncertainty surrounding the estimated relationship.\nWe can visualize this uncertainty by shading an area around the mean of all models. Here, I use +/- two standard deviations which encompasses 99.7% of all observed values. Setting the opacity alpha to a low level creates a shade rather than solid color.\nmean, std = d[\"line_y\"].mean(axis=0), d[\"line_y\"].std(axis=0)\nax[\"C\"].plot(d[\"line_x\"], mean, color=\"black\")\nax[\"C\"].fill_between(d[\"line_x\"], mean + 2 * std, mean - 2 * std, alpha=0.1, color=\"black\")\nax[\"C\"].set(ylabel=\"Mean amplitude [$\\mu$V]\", xlabel=\"Elevation [$^\\circ$]\")\nFinally, we can move the y-axis ticks and label to the right so it does not overlap with other subplots\nax[\"C\"].yaxis.tick_right()\nax[\"C\"].yaxis.set_label_position(\"right\")\n\n\n\nPlot 4\n\n\n\n\nLabeling subplots\nThe only thing left to do is labeling the panels so we can refer to them in text. One practical option is to use the keys from the ax dictionary as labels and put them at the same location relative to each axis. Because the text() method treats x and y values as data coordinates per default, we have to use the transAxes transform.\nfor label in ax.keys():\n    if label.isupper():\n        ax[label].text(-0.06, 1, label, transform=ax[label].transAxes, font='bold')\n\n\n\nPlot 5\n\n\n\n\nSaving\nWhen saving the figure as an image, make sure to use a sufficiently high number of dots per inches (dpi) so it looks nice printed. Also, don‚Äôt change the image‚Äôs height or width after saving the image because it will make the font size vary across figures. Instead use the figsize attribute to create figures of the desired size. It is possible to remove the margins by setting bbox_inches=\"tight\" but be aware that this may change the defined figsize.\nfig.savefig('awesome_figure.png', dpi=300, bbox_inches='tight'\nNow the figure is ready for publication!\n\n\n\n\n Back to top"
  }
]